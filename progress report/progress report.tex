\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage[scr]{rsfso}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{natbib}
%\usepackage{glossaries}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\usepackage[symbols,nogroupskip,sort=none]{glossaries-extra}

\glsxtrnewsymbol[description={Individual index}]{i}{\ensuremath{i}}
\glsxtrnewsymbol[description={Family (Cluster) index}]{j}{\ensuremath{j}}
\glsxtrnewsymbol[description={Proband index}]{p}{\ensuremath{p}}
\glsxtrnewsymbol[description={Number of events in family $j$}]{dj}{$d_j$}
\glsxtrnewsymbol[description={Some time}]{t}{\ensuremath{t}}
\glsxtrnewsymbol[description={Some Time for the proband}]{a}{\ensuremath{a}}
\glsxtrnewsymbol[description={Event Time}]{T}{\ensuremath{T}}
\glsxtrnewsymbol[description={Event indicator for individual $i$ in family $j$}]{delta}{$\delta_{ij}$}
\glsxtrnewsymbol[description={The observed survival data ($t,\delta$)}]{w}{\ensuremath{w}}
\glsxtrnewsymbol[description={Number of individuals}]{n}{\ensuremath{n}}
\glsxtrnewsymbol[description={Number of Families (Clusters)}]{J}{\ensuremath{J}}
\glsxtrnewsymbol[description={Index of the sampled completed dataset in the MCEM}]{m}{\ensuremath{m}}
\glsxtrnewsymbol[description={Number of the sampled completed dataset in the MCEM}]{M}{\ensuremath{M}}
\glsxtrnewsymbol[description={Frailty term}]{z}{\ensuremath{z}}
\glsxtrnewsymbol[description={$q$-th element of Gauss Hermite Quadrature}]{q}{\ensuremath{q}}
\glsxtrnewsymbol[description={$q$-th weight of Gauss Hermite Quadrature}]{omega}{\ensuremath{\omega}}
\glsxtrnewsymbol[description={$q$-th node of Gauss Hermite Quadrature}]{yq}{\ensuremath{y_q}}
\glsxtrnewsymbol[description={Total number of quadratures}]{Nq}{\ensuremath{N_q}}
\glsxtrnewsymbol[description={Hazard fucntion}]{h}{\ensuremath{h(\cdot)}}
\glsxtrnewsymbol[description={Baseline hazard function}]{h_0}{\ensuremath{h_0(\cdot)}}
\glsxtrnewsymbol[description={Cumulative hazard fucntion}]{H}{\ensuremath{H(\cdot)}}
\glsxtrnewsymbol[description={Survival fucntion}]{S}{\ensuremath{S(\cdot)}}
\glsxtrnewsymbol[description={Ascertainment of family $j$ into the study}]{A}{$A_j(\cdot)$}
\glsxtrnewsymbol[description={Likelihood function}]{L}{$L(\cdot)$}
\glsxtrnewsymbol[description={Log-likelihood function}]{ell}{$\ell(\cdot)$}
\glsxtrnewsymbol[description={Laplace transform}]{lap}{$\mathscr{L}(\cdot)$}
\glsxtrnewsymbol[description={Covariates}]{X}{\ensuremath{\mathbf{x}}}
\glsxtrnewsymbol[description={Model coefficients vector}]{beta}{$\boldsymbol{\beta}$}
\glsxtrnewsymbol[description={Parameter vector}]{theta}{$\boldsymbol{\theta}$}
\glsxtrnewsymbol[description={The combination of $(\boldsymbol{\beta}, \lambda, \alpha)$}]{Lambda}{$\Lambda$}
\glsxtrnewsymbol[description={Weibull shape parameter}]{lambda}{$\lambda$}
\glsxtrnewsymbol[description={Weibull scale parameter}]{alpha}{$\alpha$}
\glsxtrnewsymbol[description={General form of the parameter in an undefined frailty distribution}]{upsilon}{$\upsilon$}
\glsxtrnewsymbol[description={Gamma shape and rate parameters}]{k}{$k$}
\glsxtrnewsymbol[description={Log-Normal variance parameter}]{sigma}{$\sigma^2$}
\glsxtrnewsymbol[description={Missing data distribution parameters}]{psi}{$\psi$}








\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argmaxA}{arg\,max}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

%%% Change the date
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\renewcommand*{\today}{Mar. 29, 2024}

\journal{Dr. Choi \& Dr. Espin-Garcia}



\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Jiaqi's Thesis Progress Report (Updated Apr. 17)}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}


\author[rvt]{Jiaqi Bi}
\ead{jbi23@uwo.ca}
%\author[rvt]{Jiaqi Bi\corref{cor1}\fnref{fn1}}
%\ead{jbi23@uwo.ca}
%\author[rvt]{E.~Deccache\fnref{fn1}}
%\ead{elciodeccache@unifei.edu.br }
%\author[rvt]{B.D.~Bonatto\fnref{fn1}}
%\ead{bonatto@unifei.edu.br }
%\author[rvt]{H.~Arango\fnref{fn1}}
%\ead{hector.arango@uol.com.br }
%\author[rvt]{E.O.~Pamplona\fnref{fn2}}
%\ead{pamplona@unifei.edu.br}
%%\author[els]{E.O.~Pamplona\corref{cor2}\fnref{fn1,fn3}}
%%\ead[url]{pamplona@unifei.edu.br}
%%\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Affliated with Schulich School of Medicine \& Dentistry, Dept. Epidemiology and Biostatistics}
%\fntext[fn1]{Study Affiliated with Department of Statistical and Actuarial Sciences}
%\cortext[cor2]{Principal corresponding author}
%\fntext[fn1]{CERIn - Center of Excellence in Smart Grids}
%\fntext[fn2]{IEPG - Management \& Production Engineering Institute \\Unifei - Federal University of Itajuba}

\address[rvt]{Western University, \\ Schulich School of Medicine \& Dentistry, \\ Department of Epidemiology and Biostatistics}

%\tnotetext[t1]{The authors would like to thank ELETROBRAS, ANEEL, INERGE, CAPES, CNPq and FAPEMIG for financially supporting this research.}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

%\begin{abstract}
%\setstretch{1.2}
%% Text of abstract

%\end{abstract}

%\begin{keyword}
%Electricity theft \sep Regulated Electricity Company \sep Economic Impact \sep Tarot \sep Operational Optimal Point.
%% keywords here, in the form: keyword \sep keyword
%Missing data \sep Multiple imputation \sep complete case analysis \sep EM algorithm \sep MCMC
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

%\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers
%\setstretch{1.5}
%% main text
\section{To Do List}
\begin{enumerate}
    \item Gibb's sampler
\end{enumerate}
\section{Notations}
\printunsrtglossary[type=symbols,style=long, title = {List of Notations}]

\section{Weibull Parametric Approach}
For the model efficiency of the analyses in a genetic research, a parametric survival analysis is usually chosen over semi-parametric survival analysis~\cite{rudolph2018parametric, berger2001statistical}.
From the beginning of the discussion, I have obtained the model, i.e., the hazard function is
\begin{equation}
    h_{ij}(t_{ij}|\mathbf{x}_{ij}, z_j)=h_0(t_{ij})\exp(\beta_1x_{1,ij}+\beta_2 x_{2,ij})z_j
\end{equation}
There are total $n_j$ individuals in family $j$, where $i=1,...,n_j$, and total $J$ families that $j=1,...,J$. $x_{1,ij}$ is the genotype, or say mutation gene status for individual $i$ in family $j$. $x_{2,ij}$
is the PRS for individual $i$ in family $j$. The frailty term $z_j$, has a pdf of $f(z)$, which can be Gamma, log-normal, or other frailty distributions.
The support of $f(z)$ is always non-negative. The Weibull baseline hazard function is defined as
\begin{equation}
    h_0(t_{ij})=\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}
\end{equation}
where $\lambda$ is the shape parameter and $\alpha$ is the rate parameter. Let $\xi_{ij}=\exp(\beta_1 x_{1,ij}+\beta_2 x_{2,ij})$, the hazard function is 
\begin{equation}
    h_{ij}(t_{ij}|\mathbf{x}_{ij}, z_j)=\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j
\end{equation}
The survival function $S(t)$ can be obtained through cumulative hazard function $H(t)$
\begin{align}
    H(t_{ij}|\mathbf{x}_{ij}, z_j)&=\int_0^{t}h_{ij}(u|\mathbf{x}_{ij}, z_j)du\\
    &=\alpha^{\lambda}\xi_{ij}z_j\lambda\int_0^t u^{\lambda-1}du\\
    &=\alpha^{\lambda}\xi_{ij}z_j\lambda\cdot \frac{1}{\lambda} t_{ij}^{\lambda}=\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda}
\end{align}
and the survival function
\begin{equation}
    S(t_{ij}|\mathbf{x}_{ij}, z_j)=\exp(-H(t_{ij}|\mathbf{x}_{ij}, z_j))=\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})
\end{equation}
Let $\boldsymbol{\theta}=\{\boldsymbol{\beta}, \alpha, \lambda, \upsilon\}$, where $\upsilon$ is the parameter for the frailty distribution of the choice. 
In our example dataset, $\boldsymbol{\beta}=(\beta_1, \beta_2)$. 
Therefore, the likelihood assuming missing data and frailties are observed can be written as
\begin{align}
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\prod_{i=1}^{n_j}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))
\end{align}
When there is no missing data but frailties are present, the frailty term can be integrated where the likelihood is taken to be the expectation with respect to the frailty $z_j$. 
The likelihood can be written as 
\begin{align}
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})f(z_j)dz_j\label{eq:frailtyonlyllhd1}\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))f(z_j)dz_j\label{eq:frailtyonlyllhd2}
\end{align}
But when the missing data and the frailty both exist in the model, we will need to account for their joint distribution within the likelihood according to~\citet{herring2002frailty}. 
\begin{align} 
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j,\mathbf{x}_{mis,ij}}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})f(z_j, \mathbf{x}_{mis,ij})dz_jd\mathbf{x}_{mis,ij}\label{eq:frailtyandmissing1}\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j,\mathbf{x}_{mis,ij}}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))f(z_j, \mathbf{x}_{mis,ij})dz_jd\mathbf{x}_{mis,ij}\label{eq:frailtyandmissing2}
\end{align}
The following section~\ref{sec:gammafr} and section~\ref{sec:lognormfr} discuss how to handle the frailty within the likelihood when there are no missing data, which are corresponding to the Equation~\ref{eq:frailtyonlyllhd1} and~\ref{eq:frailtyonlyllhd2}. 
The section~\ref{sec:likeandmissing} will discuss how to handle the frailty and the missing data jointly, which is corresponding to the likelihood equation~\label{eq:frailtyandmissing1} and~\ref{eq:frailtyandmissing2}.

\section{Ascertainment Correction} 
Within a genetic study, those families are typically selected when there is an affected person called a proband. This will yield a selection bias because this is no long a case-control study, and can potentially defect the statistical power~\cite{park2015adjusting, clark2005ascertainment}.
It is crucial to address the ascertainment bias. Consider $A$ as the event of being ascertained, $D$ as the data, we then have $P(D, A|\boldsymbol{\theta})=P(A|D,\boldsymbol{\theta})P(D|\boldsymbol{\theta})$. 
Also, we know $A$ is included in $D$, from Baye's rule
\begin{equation} 
    P(D|\boldsymbol{\theta})= \frac{P(D,A|\boldsymbol{\theta})}{P(A|D, \boldsymbol{\theta})}\propto\frac{L(\boldsymbol{\theta}|D)}{P(A|D,\boldsymbol{\theta})}
\end{equation}
For each family $j$, the ascertainment $A_j$ is defined to be the probability of the proband $p$ being ascertained by the age $a_{p_j}$ at examination, i.e., $A_j=P(T_{p_j} < a_{p_j})$ where $a_{p_j}$ is proband's age at study entry. Applying the ascertainment correction for the log-likelihood in family $j$: 
\begin{equation}
    \tilde{\ell}_{j}(\boldsymbol{\theta})=\ell_j(\boldsymbol{\theta})-\log A_j(\boldsymbol{\theta})
\end{equation}
where $\tilde{\ell}$ is the log-likelihood with ascertainment correction, and $\ell$ is the crude log-likelihood. 
Define $\mathbf{x}_{p_j}$ the covariates for proband in family $j$, so we can further write the formula for the ascertainment correction within different frailty models. 

\section{Gamma Frailty}\label{sec:gammafr}
We can obtain the likelihood for Gamma frailty model following the instruction by~\citet{balan2020tutorial}. 
The Laplace transform of the frailty $z\sim\text{Gamma}(k, k)$, for the simplicity of the mathematical expression, the following Laplace transform will ignore the subscript, denote $\mathscr{L}(f(z))=\phi(s)$ where $s=\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})$:
\begin{align}
    \phi(s)&=\int_0^{\infty}e^{-sz}f(z)dz\\
    &=\int_0^{\infty}e^{-sz}\frac{k^k}{\Gamma(v)}z^{k-1}e^{-kz}dz
\end{align}
Using the Gamma property: $\int_0^{\infty}z^{n-1}e^{-az}dz=\frac{\Gamma(n)}{a^n}$, $\phi(s)$ can be further written as
\begin{equation}
    \phi(s)=\frac{k^k}{\Gamma(k)}\int_0^{\infty}e^{-(s+k)z}z^{k-1}dz=\frac{k^k}{\Gamma(k)}\cdot \frac{\Gamma(k)}{(s+k)^k}=(1+\frac{s}{k})^{-k}
\end{equation}
The second derivative is $\frac{d^2\phi(s)}{ds^2}=\int_0^{\infty}(-z)^2e^{-sz}f(z)dz$. 

\noindent
The third derivative is $\frac{d^3\phi(s)}{ds^3}=\int_0^{\infty}(-z)^3e^{-sz}f(z)dz$, ...
Therefore, its $d$-th derivative, denote $\phi(s)^{(d)}$:
\begin{align}
    \phi(s)^{(d)}&=(-1)^d\int_0^{\infty}z^de^{-sz}f(z)dz\\
    &=(-1)^d\frac{(k+d-1)!}{(k-1)!(s+k)^d}(1+\frac{s}{k})^{-k}
\end{align}
Let $\boldsymbol{\theta}=(\beta_1, \beta_2, \alpha, \lambda, k)$ for Gamma frailty model, the log-likelihood is then written as
\begin{align}
    \ell(\boldsymbol{\theta})&=\sum_{j=1}^k\log \left [ \int_0^{\infty}\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}, z_j))^{\delta_{ij}}\exp (-H(t_{ij}|\mathbf{x}_{ij}, z_j))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\int_{0}^{\infty}\prod_{i=1}^{n_j}(z_j h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\exp(-z_j H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\int_0^{\infty}z_j^{d_j}\exp(-z_j\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j \right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\frac{(k+d_j-1)!}{(k-1)!(\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})+k)^{d_j}}\Big(1+\frac{\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})}{k}\Big)^{-k}\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}((h(t_{ij}|\mathbf{x}_{ij}) )^{\delta_{ij}})\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\log\left [h(t_{ij}|\mathbf{x}_{ij})^{\delta_{ij}} \frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\left [\sum_{i=1}^{n_j}(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij})) + \log\Big (\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j}\Big )\right ]
\end{align}
 Note we can still apply Laplace transform for the ascertainment correction, such that
\begin{align}
    A_j(\boldsymbol{\theta})&=1-S_{p_j}(a_{p_j}|\mathbf{x}_{p_j})\\
    &=1-\int_0^{\infty} S_{p_j}(a_{p_j}|\mathbf{x}_{p_j},z_j)f(z_j)dz_j\\
    &=1-\int_0^{\infty}\exp(-z_j\cdot H_{p_j}(a_{p_j}|\mathbf{x}_{p_j}))f(z_j)dz_j\\
    &=1-(1+\frac{H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})}{k})^{-k}
\end{align}
\section{Log-Normal Frailty}\label{sec:lognormfr}
The log-normal frailty is not the power-variance-function (PVF) family, so there is no closed form for Laplace transform or expressions for survivors. But we are able to estimate the Laplace transform using Gauss Hermite Quadrature. We typically standardize the log-normal frailty $Z$ as
\begin{align} 
    E(\log Z)&=0\\
    \text{Var}(\log Z)&=\sigma^2
\end{align} 
That is, $z\sim \text{log-Normal}(0, \sigma^2)$. The probability density function $f(z)$ is then
\begin{equation}\label{eq:lognormalfrailty}
    f(z)=\frac{1}{\sqrt{2\pi}\sigma}z^{-1}\exp (-\frac{\log (z)^2}{2\sigma^2})
\end{equation}
The Laplace transform is then
\begin{equation}
    \phi(s)=\mathscr{L}(f_Z)(s)=\int_0^{\infty}\exp(-sz)\cdot f(z)dz
\end{equation}
Using variable transformation, let $y=\frac{\log(z)}{\sqrt{2}\sigma}$, then $z=\exp(\sqrt{2}\sigma y)$, and $dz=\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy$. Therefore, for $d$-th derivative:
\begin{align}
    \phi(s)^d&=\int_{-\infty}^{\infty}z^d\exp(-sz)\cdot\frac{1}{\exp(\sqrt{2}\sigma y)\sigma\sqrt{2\pi}}\cdot\exp(-y^2)\cdot\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy\\
    &=\int_{-\infty}^{\infty}\exp(\sqrt{2}\sigma y)^d\exp(-s\exp(\sqrt{2}\sigma y))\cdot\frac{1}{\sqrt{\pi}}\exp(-y^2)dy
\end{align}
\begin{defn}[Gauss-Hermite Quadrature]\label{defn:gausshermite}
    The integrand part can be solved using Gauss-Hermite Quadrature. In numerical analysis, the method can be applied in the following form:
\begin{equation}
    \int_{-\infty}^{\infty}\exp(-x^2)f(x)dx\approx \sum_{i=1}^n\omega_i f(x_i)
\end{equation}
where $n$ is number of sample points used, and $x_i$ is the roots of Hermite polnomial $H_n (x)$ such that $i=1, ..., n$, and the weights $\omega_i$ is 
\begin{equation}
    \omega_i=\frac{2^{n-1}n!\sqrt{n}}{n^2[H_{n-1}(x_i)]^2}
\end{equation}
\end{defn}

\noindent
Applying Definition~\ref{defn:gausshermite}, the integral of the Laplace transform is then
\begin{equation}
    \phi(s)^d=\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\omega_{q}\exp(-s\exp(\sqrt{2}\sigma y_{q}))\exp(\sqrt{2}\sigma y_{q})^d
\end{equation}
where $q$ denotes the $q$-th element of Gauss Hermite Quadrature, i.e., $\omega_{q}$ denotes the $q$-th weight, $y_{q}$ denotes the $q$-th node, and $N_{q}$ denotes the total number of quadratures. Thus, substituting into the log-likelihood:
\begin{equation}
    \ell_j(\boldsymbol{\theta})=\sum_{i=1}^{n_j}\delta_{ij}\log(h(t_{ij}|\mathbf{x}_{ij}))+\log\Big (\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\left [\omega_{q}\exp(\sqrt{2}\sigma y_{q})^{d_j}\exp\Big (-\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})\exp(\sqrt{2}\sigma y_{q})\Big )\right ]\Big )
\end{equation}
Similarly, the ascertainment correction in the log-normal frailty can be written as 
\begin{align}
    A_j(\boldsymbol{\theta})&=1-\int_{-\infty}^{\infty} \exp(-z H(a_{p_j}|\mathbf{x}_{p_j}))f(z)dz\\
    &=1-\sum_{q=1}^{N_{q}}\omega_{q} \exp\left (-(\sum_{i=1}^{n_j} H(a_{p_j}|\mathbf{x}_{p_j}))\exp (\sqrt{2}\sigma y_{q_p})\right )
\end{align}

\section{Likelihood and Missing Data}\label{sec:likeandmissing}
\subsection{Reviews on Missing Data}
In this subsection, the notations are \textbf{distinct} to all other sections or subsections. 
The missing data problem was firstly brought by~\citet{rubin1976inference}, and further targetted as a major statistical problem which many methodologists have developed different statistical tools to handle the missing data. 
Such as the practical book written by~\citet{Rubin1987}, and some comprehensive reviews on current missing data problems by~\citet{baraldi2010introduction}. 
The missing data mechanism was introduced by~\citet{little2019statistical}. 
There are three missing data mechanisms, which are Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR). 
There are some reviews on the missing data which rigorously present the statistical concept of three types of the missing mechanism~\cite{santos2019generating}.
\begin{defn}(MCAR)\label{defn:MCAR}
Denote $Y$ as the complete data matrix, and $M$ as the missing data indicator matrix. 
Define $y_{ij}$ and $m_{ij}$ as $i$-th row (observation) and $j$-th column (variable) for the matrix $Y$ and $M$. 
The conditional distribution of the missingness is said to be 
\begin{equation} 
    f(m_i|y_i, \phi)=f(m_i|\phi)
\end{equation}
That is, for the parameters of this distribution, $m_i$ does not depend on any observed or missing data.
\end{defn}
\begin{eg}(MCAR Example)\label{eg:MCARExample}
    There is a blind box with 500 indexed balls (No. 1 to 500) and their weights are unknown. 
    We randomly draw 100 balls and measure their weights and record them in the Excel file. 
    The Excel file contains two columns called Index and Weight, only those randomly selected balls will have Weights being filled. 
    Those weights of unselected balls are called MCAR. 
\end{eg}
\begin{defn}(MAR)\label{defn:MAR}
    Denote $y_{i,obs}$ as the observed $y$, and $y_{i,mis}$ as the missing $y$. 
    Note that $y_i=(y_{i,obs},y_{i,mis})$. 
    The missing component is defined to be MAR if $m$ only dependes on $y_{i,obs}$. 
    That is,

    \begin{equation} 
        f(m_i|y_i,\phi)=f(m_i|y_{i,obs}, \phi)
    \end{equation}
\end{defn}
\begin{eg}(MAR Example)\label{eg:MARExample}
    In a psychological study, participants are asked to complete a survey so the scientist can profile their personalities. 
    One question that asks participants to report their Mood status being good or bad. 
    Male participants are typically too shy to answer this question, which yields some responses being missing. 
    This is called the MAR, that the missingness on Mood status depends on the participant's gender, but not on the missing Mood itself.
\end{eg}
\begin{defn}(MNAR)\label{defn:MNAR}
    In the MNAR, the missingness depends on the missing data itself, which is 
    \begin{equation} 
        f(m_i|y_i,\phi)=f(m_i|y_{i,mis}, y_{i,obs}, \phi)
    \end{equation}
    In this case, the analysis needs to be conducted with caution. 
    The missingness should be included in the likelihood construction.
\end{defn}
\begin{eg}(MNAR Example)\label{eg:MNARExample}
    There is a study on participants' incomes. 
    Person A makes \$200,000 per year, so they decide to report this amount without hesitancies. 
    Person B makes \$10,000 per year, so they are not willing to provide this information, which this response is left as blank. 
    This type of missing depends on the missing data itself, that Person B refuses to provide the response due to the response being comparatively low. 
\end{eg}
\subsection{Without Considering the Kinship Structure}\label{sec:without}
When assuming the data are missing at random, the missingness is only associated to the observed data. 
The frailty term and the missing data are therefore assumed independent. 
Denote $w_{ij} = (t_{ij}, \delta_{ij})$ be the observed survival data.
From the complete log-likelihood:

\begin{align} 
    \ell_C(\boldsymbol{\theta})&=\sum_{j=1}^J\sum_{i=1}^{n_j}\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}, z_j) - H(t_{ij}|\mathbf{x}_{ij}, z_j)\\
    &-\sum_{j=1}^J \log (1-S_{p_j}(a_{p_j}|\mathbf{x}_{p_j}, z_j)) \\
    &= \sum_{j=1}^J\sum_{i=1}^{n_j}\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij})z_j - H(t_{ij}|\mathbf{x}_{ij})z_j \\
    &- \sum_{j=1}^{J} \log(1- \exp(z_j H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})))
\end{align}

In the MCEM framework, define $\boldsymbol{\theta}^{(r)}$ as $r$-th updates. 
The E-step can be written as 

\begin{align} 
    E(\ell_C(\boldsymbol{\theta})|\boldsymbol{\theta}^{(r)})=&\sum_{j=1}^J\sum_{i=1}^{n_j}\int_{\mathbf{x}_{mis}, z_j}\Big (\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}, z_j) - H(t_{ij}|\mathbf{x}_{ij}, z_j)\Big )\\
    &\times f(\mathbf{x}_{ij,mis}, z_j|\mathbf{x}_{obs,ij}, \boldsymbol{\theta}^{(r)})d\mathbf{x}_{ij,mis}dz_j\\
    &-\sum_{j=1}^J \int_{\mathbf{x}_{mis}, z_j} \log(1- \exp(z_j H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})))\\
    &\times f(\mathbf{x}_{ij,mis}, z_j|\mathbf{x}_{obs,ij}, \boldsymbol{\theta}^{(r)})d\mathbf{x}_{ij,mis}dz_j
\end{align}

such that we need to integrate out the joint density of the frailty term and the missing covariate from $f(\mathbf{x}_{ij,mis}, z_j|\mathbf{x}_{obs,ij}, \boldsymbol{\theta}^{(r)})$. 
There are selections of the frailty density, such as Gamma distribution, log-normal distribution, and etc which have been discuseed in the previous chapter.  
In general, let's write $f(z_j|\upsilon)$ for the frailty distribution may be chosen with some parameters $\upsilon$. 
Proposed by~\citet{herring2002frailty}, the joint distribution of the frailty and the missing data can be adapted in our scenario that accounting for the ascertainment:

\begin{align} 
    f(\mathbf{x}_{mis,ij}, z_j|\mathbf{x}_{obs,ij}, w_{ij}, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})&=f(\mathbf{x}_{mis,ij}|\mathbf{x}_{obs,ij}, w_{ij}, z_j, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})\\
    &\times f(z_j|\mathbf{x}_{obs,ij}, w_{ij}, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})
\end{align}

we define $\Lambda = (\boldsymbol{\beta}, \alpha, \lambda)$, then further we can write

\begin{align} 
    &f(\mathbf{x}_{mis,ij}, z_j|\mathbf{x}_{obs,ij}, w_{ij}, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})\\
    &=\frac{f(w_{ij}|\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}, z_j, T_{p_j}<a_{p_j}, \Lambda^{(r)})f(\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}|\psi^{(r)})f(z_j|\upsilon^{(r)})}{\int_{\mathbf{x}_{mis,ij},z_j}f(w_{ij}|\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}, z_j, T_{p_j}<a_{p_j}, \Lambda^{(r)})f(\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}|\psi^{(r)})f(z_j|\upsilon^{(r)})dz_jd\mathbf{x}_{mis,ij}}\\
    &\propto f(z_j|\upsilon^{(r)})\prod_{i=1}^{n_j}f(w_{ij}|\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}, z_j, T_{p_j}<a_{p_j}, \Lambda^{(r)})f(\mathbf{x}_{mis,ij}|\mathbf{x}_{obs,ij}, \psi^{(r)})
\end{align}

Clearly, we know $f(w_{ij}|\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}, z_j, T_{p_j}<a_{p_j}, \boldsymbol{\beta}^{(r)})$ is the likelihood of one single observation $i$ in family $j$, also we know the distribution of $f(\mathbf{x}_{ij}|\psi)$, as well as the frailty distribution $f(z_j|\upsilon)$. 
Therefore, in our case, we can apply Gibb's sampler where 

\begin{enumerate} 
    \item We sample the missing data first, which we can obtain that 
    \begin{align} 
        f(\mathbf{x}_{mis,ij}|\mathbf{x}_{obs,ij}, w_{ij}, z_j, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})&\propto f(w_{ij}|\mathbf{x}_{mis,ij}, \mathbf{x}_{obs,ij}, z_j, T_{p_j}<a_{p_j}, \Lambda^{(r)})\\
        &\times f(\mathbf{x}_{mis,ij}|\mathbf{x}_{obs,ij},\psi^{(r)})
    \end{align}
    In this case, the missing data will be filled for each iteration $r$, that being said, all data will be ``observed" in this case. 
    Therefore, we will use $\mathbf{x}_{ij}$ to simply denote completed covariates. 
    \item In order to approach the joint distribution, we now need to sample the frailty $z_j$ from 
    \begin{align} 
        f(z_j|\mathbf{x}_{ij}, w_{ij}, T_{p_j}<a_{p_j}, \boldsymbol{\theta}^{(r)})\propto \prod_{i=1}^{n_j}f(w_{ij}|\mathbf{x}_{ij}, T_{p_j}<a_{p_j}, \Lambda^{(r)})\times f(z_j|\upsilon^{(r)})
    \end{align}
    \item The Gibb's sampler has been proven as an efficient sampling method to closely approach the desired joint distribution \cite{gelfand1990sampling}. 
    We can get a frailty distribution based on what we have sampled for the missing data, and we can obtain the missing data distribution based on what we have sampled from the frailty distribution. 
\end{enumerate}
These conditional densities can be explicitly written.
We will obtain $M$ completed dataset based on the Gibb's sampler. 
In general, without the specification of the frailty distribution, the E-step in MCEM can be written as 
\begin{align} 
    Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)})&=\sum_{j=1}^J \frac{1}{M_j}\sum_{m=1}^{M_j}\sum_{i=1}^{n_j} \Big ( \delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}^{(m)}, z_j^{(m)}) - H(t_{ij}|\mathbf{x}_{ij}^{(m)}, z_j^{(m)})\Big )\\
    &+\sum_{j=1}^J\frac{1}{M_j}\sum_{m=1}^{M_j}\log(1- \exp(z_j H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})))\\
    &+\sum_{j=1}^J\frac{1}{M_j}\sum_{m=1}^{M_j}\sum_{i=1}^{n_j}\log f(\mathbf{x}_{ij,mis}^{(m)}, z_j^{(m)}|\mathbf{x}_{obs,ij}, \boldsymbol{\theta})\\
    &=\sum_{j=1}^J \frac{1}{M_j}\sum_{m=1}^{M_j}\sum_{i=1}^{n_j} \Big ( \delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}^{(m)}, z_j^{(m)}) - H(t_{ij}|\mathbf{x}_{ij}^{(m)}, z_j^{(m)})\Big )\\
    &+\sum_{j=1}^J\frac{1}{M_j}\sum_{m=1}^{M_j}\log(1- \exp(z_j H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})))\\
    &+\sum_{j=1}^J\frac{1}{M_j}\sum_{m=1}^{M_j}\sum_{i=1}^{n_j}\log f(\mathbf{x}_{mis,ij}^{(m)}|\mathbf{x}_{obs,ij}, \psi)+\sum_{j=1}^J\frac{1}{M_j}\sum_{m=1}^{M_j}\sum_{i=1}^{n_j}\log f(z_j^{(m)}|\upsilon)
\end{align} 
Note that in this case, the distribution of the missing data is univariate since we are imputing each individual $i$ in family $j$. 
However, in a genetic study, some missing covariates may need some considerations of the multivariate structure such as the kinship matrix. 



\subsection{Considering the Kinship Matrix}
When we want to include the kinship matrix into the consideration, the distribution of the missing data becomes a multivariate distribution for family $j$. 
Denote $f(\mathbf{x}_{mis,j}|\mathbf{x}_{obs,j},\boldsymbol{\psi})$ as the multivariate distribution of the missing data in family $j$. 
It is important to obtain the conditional distribution for each individual $i$ conditioning on other individuals $-i$ within family $j$. 
Assume a $n_j$ dimensional multivariate normal distribution of $\mathbf{x}_{mis}=(\mathbf{x}_{mis,1}, ...,\mathbf{x}_{mis,n_j})$ in family $j$, the index here will ignore the family index since if it's global, it will work too. 
The multivariate distribution will be assumed a normal, because we are focusing on the missing PRS which has a normal distributed behavior. 
The mean vector of this multivariate normal distribution can be written as $\boldsymbol{\mu}=(\mu_1,...,\mu_{n_j})$ and the covariance matrix $\boldsymbol{\Sigma}$. 
Note that $\boldsymbol{\Sigma}=\tilde{\psi}_g^2K+\tilde{\psi}_e^2$ and $K$ is the kinship matrix with the diagonal of 1. $\tilde{\psi}_g^2$ is the genetic variance and $\tilde{\psi}_e^2$ is the residual variance. 
If we want to find the conditional distribution of each $x_{mis,i}$ in family $j$, given others $X_{-i}$ where $X_{-i}$ is the vector of all other vairables except $x_{mis,i}$. 
Partition the mean vector and the covariance matrix, suppose $\mathbf{X}=(x_i, X_{-i})$, then partition $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ accordingly: 
\begin{equation} 
    \boldsymbol{\mu}=\begin{pmatrix} 
        \mu_i \\
        \boldsymbol{\mu}_{-i}
    \end{pmatrix}
\end{equation}
and 
\begin{equation} 
    \boldsymbol{\Sigma}=\begin{pmatrix} 
        \Sigma_{ii} & \boldsymbol{\Sigma}_{i,-i} \\
        \boldsymbol{\Sigma}_{-i,i} & \boldsymbol{\Sigma}_{-i,-i}
    \end{pmatrix}
\end{equation}
where $\Sigma_{ii}$ is actually the variance of $x_i$.  
$\boldsymbol{\Sigma}_{i,-i}$ and $\boldsymbol{\Sigma}_{-i,i}$ are transpose of each other, and are covariances between $x_i$ and $X_{-i}$. 
$\boldsymbol{\Sigma}_{-i,-i}$ is the covariance matrix of $X_{-i}$. 
Also, $\boldsymbol{\mu}$ can be estimated using a linear regression from a multivariate version with flexible covariance matrix introduced by~\citet{ziyatdinov2018lme4qtl}.
Then we can compute the conditional mean and variance from 
\begin{equation} 
    E(x_i|X_{-i})=\mu_i + \boldsymbol{\Sigma}_{i,-i}\boldsymbol{\Sigma}_{-i,-i}^{-1} (\mathbf{x}_{-i} - \boldsymbol{\mu}_{-i})
\end{equation}
and the variance 
\begin{equation} 
    \text{Var}(x_i|\mathbf{x}_{-i})=\Sigma_{ii} - \boldsymbol{\Sigma}_{i,-i}\boldsymbol{\Sigma}_{-i,-i}^{-1}\boldsymbol{\Sigma}_{-i,i}
\end{equation}
Once we can calculate these statistics, we are able to sample each $x_i$ using a univariate normal distribution while still considering the kinship matrix. 
The MCEM will then perform using the same idea of the previous subsection. 

\section{Detailed Implementations of Gibb's Sampler}
In section~\ref{sec:without}, the posterior distribution is not easy to sample from. 
So the Gibb's sampler needs an additional step called Matropolis-Hastings step. 
In the article by~\citet{herring2002frailty}, the choice of the frailty distribution is multivariate normal, which satisfies the property of log-concavity for adaptive rejection algorithm. 
However, when the frailty distribution is designed to be Gamma or log-normal distribution, the log-concavity fails. 
One may use the Metropolis-Hastings (MH) step within Gibbs to determine the acceptance or rejection when sampling the posterior distributions~\cite{griffin2013adaptive}. 
There are some articles on the MH algorithm and how it works such as by~\citet{10.1214/105051606000000286}, and some articles have discussed how Gibb's sampler are adapted using MCMC methods~\cite{10.1214/11-AAP806}. 
This section will discuss the MH-within-Gibbs algorithm when sampling the missing data and frailty. 
To sample the frailty using the Gibb's sampler with MH algorithm, the procedure is 
\begin{enumerate} 
    \item We have a proposal sampling distribution $q(z_j'|z_j)$, which represents what we are sampling in the iteration $r$
    \item We first sample from this $q(z_j'|z_j)$
    \item For $z_j$, $q(z_j'|z_j)=f(z_j|\upsilon^{(r)})$
    \item Then calculate the acceptance ratio: 
          \begin{equation} 
            \gamma=\min \left (1,\frac{\prod_{i=1}^{n_j}f(w_{ij}|\mathbf{x}_{ij}, z_j', T_{p_j}<a_{p_j}, \Lambda^{(r)})\times q(z_j'|z_j)}{\prod_{i=1}^{n_j}f(w_{ij}|\mathbf{x}_{ij}, z_j, T_{p_j}<a_{p_j}, \Lambda^{(r)})\times q(z_j|z_j')} \right )
          \end{equation}
    \item Now we are sampling $\tilde{u}\sim \text{Unif}(0,1)$, and we accept $z'$ if $\tilde{u}\leq \gamma$. Otherwise, reject and set $z_j'=z_j$. 
\end{enumerate}\
The same idea for missing data. 
This MH-within-Gibbs is more flexible when log-concavity does not hold for the posterior distribution, also when it's impossible to direct sample from the posterior distribtion. 
\section{Multiple Imputation}
In the MCEM, the likelihood is explicitly written out and it ``automatically'' solves the missing data and complete data within an iterative process to make the inference of model parameters. 
However, there are several disadvantages such as a highly costly computational time, since parameters are estimated all at once based on the likelihood equation. 
Moreover, there are arguments that during the MCEM, the precision is questionable because there is no distinction between the observed and imputed data, although their weights are evaluated in the E-step since the integral was broken down to a summation~\cite{van2011multiple}. 
There are also selections of multiple imputation methods, which the most common way is to use the linear regression using Bayesian framework to make imputations on continuous variables, same idea of using logistic regression for binary variables~\cite{Rubin1987}. 

\subsection{Multiple Imputation for the Continuous Variable without Considering the Family Structure}
Again, to avoid overloaded mathematical notations, this subsection will not follow the previously defined notations. 
When making the imputation on the continuous variable, one easy way is to assume a conditionally normal model. 
Suppose $y$ is the variable contains missing values, $\mathbf{x}$ are other variables are fuly observed. 
We assume there are total $p$ parameters being estimated in this linear regression. 
The conditionally normal model (linear regression) can be written as 
\begin{equation} 
    y|\mathbf{x},\boldsymbol{\beta}\sim N(\boldsymbol{\beta}\mathbf{x}, \sigma^2)
\end{equation}
In the linear regression setting, 
\begin{equation} 
    y=\mathbf{x}\boldsymbol{\beta}+\epsilon,~ \epsilon\sim N(0, \sigma^2)
\end{equation}
This simply corresponds to the likelihood 
\begin{equation}\label{eq:bayeslinearnormal}
    f(y|\mathbf{x}, \boldsymbol{\beta}, \sigma^2)\propto (\sigma^2)^{-n/2}\exp \Big (-\frac{1}{2\sigma^2} (y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})\Big )
\end{equation}
because of the conditional normality of $y$. 
We can solve that 
\begin{equation} 
    \hat{\boldsymbol{\beta}}=(\mathbf{x}^{\top}\mathbf{x})^{-1}\mathbf{x}^{\top}y
\end{equation}
In the Bayesian framework, we need to find the prior, which is the joint distribution $f(\sigma^2,\boldsymbol{\beta})$. 
Note that $(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})$ can be written as 
\begin{align} 
    (y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})&=\big ((y-\mathbf{x}\boldsymbol{\beta})+(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\big )^{\top}\big ((y-\mathbf{x}\boldsymbol{\beta})+(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\big )\\
    &=(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})+2(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\\
    &=(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})
\end{align}
So Equation~\ref{eq:bayeslinearnormal} will become 
\begin{equation} 
    f(y|\mathbf{x}, \boldsymbol{\beta}, \sigma^2)\propto \underbrace{(\sigma^2)^{-v/2}\exp (-\frac{vs^2}{2\sigma^2})}_{f(\sigma)}\underbrace{(\sigma^2)^{-\frac{n-v}{2}}\exp \Big (-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})\Big )}_{f(\boldsymbol{\beta}|\sigma)}
\end{equation}
where $vs^2=(\mathbf{y}-\mathbf{x}\hat{\boldsymbol{\beta}})^{\top}(\mathbf{y}-\mathbf{x}\hat{\boldsymbol{\beta}})=SSE$, such that $v=n_{obs}-p$. 
Then $f(\sigma^2)$ can be written as a proportional density to the inverse gamma distribution,
\begin{equation} 
    f(\sigma^2)\propto (\sigma^2)^{-\frac{v}{2}-1}\exp (-\frac{vs^2}{2\sigma^2})=(\sigma^2)^{-\frac{v}{2}-1}\exp (-\frac{SSE}{2\sigma^2})
\end{equation}
In this $\sigma^2\sim \text{Inverse-Gamma}(\alpha, \phi)$, we have $\alpha = \frac{v}{2}=\frac{n_{obs}-p}{2}$ and $\phi = \frac{1}{2}vs^2=\frac{SSE}{2}$.
From the inverse-gamma property, when 
\begin{equation} 
    \sigma^2\sim \text{Inverse-Gamma}(\frac{n_{obs}-p}{2}, \frac{SSE}{2})
\end{equation}
and $\exists\lambda$ such that 
\begin{equation} 
    \sigma^2=\frac{SSE}{2}/\lambda
\end{equation}
then $\lambda$ can be transformed 
\begin{equation} 
    \lambda = \frac{\chi^2_{n_{obs}-p}}{2}
\end{equation}
since $\chi^2_{df}=\text{Gamma}(\frac{df}{2},2)$, so 
\begin{equation} 
    \sigma^2=\frac{\frac{SSE}{2}}{\frac{\chi^2_{n_{obs}-p}}{2}}=\frac{SSE}{\chi^2_{n_{obs}-p}}
\end{equation}
Thus, we can sample the standard deviation of the missing data distribution from 
\begin{equation} 
    \sigma^*=\hat{\sigma}\sqrt{\frac{SSE}{\chi^2_{n_{obs}-p}}}=\hat{\sigma}\sqrt{\frac{SSE}{g}}
\end{equation}
from sampling $g\sim \chi^2_{n_{obs}-p}$. 
Moreover, we know 
\begin{equation} 
    \text{Var}(\hat{\boldsymbol{\beta}})=\sigma^2(\mathbf{x}^{\top}\mathbf{x}^{-1})=\mathbf{V}
\end{equation}
so the marginal distribution for $\boldsymbol{\beta}$, 
\begin{equation}\label{eq:betadistmi}
    \boldsymbol{\beta}\sim N(\hat{\boldsymbol{\beta}}, \sigma^2(\mathbf{x}^{\top}\mathbf{x})^{-1})
\end{equation}
Note that when a random variable $T\sim N(\mathbf{m}, \mathbf{c})$, then $T$ can be generated from a standard normal variable $\mathbf{u}$ with 
\begin{equation} 
    T=\mathbf{m}+\mathbf{L}\mathbf{u}
\end{equation}
where $\mathbf{L}$ is the cholesky decomposition of $\mathbf{c}$ such that $\mathbf{c}=\mathbf{L}\mathbf{L}^{\top}$.
For $\boldsymbol{\beta}$, from~\ref{eq:betadistmi}, it can be derived as 
\begin{align} 
    \boldsymbol{\beta}&=\hat{\boldsymbol{\beta}}+\sigma (\mathbf{x}^{\top}\mathbf{x})^{-1/2}\mathbf{u}_1\\
    &=\hat{\boldsymbol{\beta}}+\mathbf{u}_1\mathbf{V}^{-1/2}
\end{align}
Adjusting for $\sigma^*$ to make sure $\boldsymbol{\beta}$ matches the variability implied by the random draw of $\sigma^*$, 
\begin{equation} 
    \boldsymbol{\beta}^*=\hat{\boldsymbol{\beta}}+\frac{\sigma^*}{\hat{\sigma}}\mathbf{u}_1\mathbf{V}^{-1/2}
\end{equation}
such that $\mathbf{u}_1$ is a row vector of $p$ independent draws from a standard normal distribution, $u_{1k}\stackrel{iid}{\sim} N(0,1)$, and $k=1,...,p$. 
The imputation for $y_i^*$ is computed as 
\begin{equation} 
    y_i^*=\boldsymbol{\beta}^*\mathbf{x}_i+u_{2i}\sigma^*,~ s.t.~ u_{2i}\sim N(0,1)
\end{equation}
where $u_{2i}$ adds the uncertainty to the imputation as well to ensure the imputation is not solely based on the predicted value of $y_i^*$. 
This prevents the underestimation of the variability. 
Therefore, the comprehensive steps of the multiple imputation on the continuous missing data can be concluded to the following steps: 
\begin{enumerate} 
    \item Calculate $\hat{y}=\hat{\boldsymbol{\beta}}\mathbf{x}$ using $y_{obs}$, and $\hat{\boldsymbol{\beta}}$ can be obtained easily, as well as $\hat{\sigma}$, and $\text{Var}(\hat{\boldsymbol{\beta}})=\mathbf{V}$
    \item Draw $g\sim \chi^2_{n_{obs}-p}$ for one random draw 
    \item Calculate $\sigma^*=\hat{\sigma}/\sqrt{SSE/g}$
    \item Draw a $p$ dimensional vector $\mathbf{u}_1$ such that $u_{1k}\stackrel{iid}{\sim} N(0,1)$ and $k=1,...,p$
\end{enumerate}

%\section{Stochastic EM?}




\newpage

%\section{References}
%\label{S.7}
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

%\bibliographystyle{elsarticle-num}
\bibliographystyle{unsrtnat}
\bibliography{progress.bib}
%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

%\begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

%\bibitem{}

%\end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.