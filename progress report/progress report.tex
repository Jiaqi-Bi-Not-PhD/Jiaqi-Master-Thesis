\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage[scr]{rsfso}
\usepackage{xcolor}
\usepackage{soul}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argmaxA}{arg\,max}
\newtheorem{defn}{Definition}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

%%% Change the date
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\renewcommand*{\today}{Mar. 8, 2024}

\journal{Dr. Choi \& Dr. Espin-Garcia}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Jiaqi's Thesis Progress Report (Updated Mar. 8)}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}


\author[rvt]{Jiaqi Bi}
\ead{jbi23@uwo.ca}
%\author[rvt]{Jiaqi Bi\corref{cor1}\fnref{fn1}}
%\ead{jbi23@uwo.ca}
%\author[rvt]{E.~Deccache\fnref{fn1}}
%\ead{elciodeccache@unifei.edu.br }
%\author[rvt]{B.D.~Bonatto\fnref{fn1}}
%\ead{bonatto@unifei.edu.br }
%\author[rvt]{H.~Arango\fnref{fn1}}
%\ead{hector.arango@uol.com.br }
%\author[rvt]{E.O.~Pamplona\fnref{fn2}}
%\ead{pamplona@unifei.edu.br}
%%\author[els]{E.O.~Pamplona\corref{cor2}\fnref{fn1,fn3}}
%%\ead[url]{pamplona@unifei.edu.br}
%%\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Affliated with Schulich School of Medicine \& Dentistry, Dept. Epidemiology and Biostatistics}
%\fntext[fn1]{Study Affiliated with Department of Statistical and Actuarial Sciences}
%\cortext[cor2]{Principal corresponding author}
%\fntext[fn1]{CERIn - Center of Excellence in Smart Grids}
%\fntext[fn2]{IEPG - Management \& Production Engineering Institute \\Unifei - Federal University of Itajuba}

\address[rvt]{Western University, \\ Schulich School of Medicine \& Dentistry, \\ Department of Epidemiology and Biostatistics}

%\tnotetext[t1]{The authors would like to thank ELETROBRAS, ANEEL, INERGE, CAPES, CNPq and FAPEMIG for financially supporting this research.}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

%\begin{abstract}
%\setstretch{1.2}
%% Text of abstract

%\end{abstract}

%\begin{keyword}
%Electricity theft \sep Regulated Electricity Company \sep Economic Impact \sep Tarot \sep Operational Optimal Point.
%% keywords here, in the form: keyword \sep keyword
%Missing data \sep Multiple imputation \sep complete case analysis \sep EM algorithm \sep MCMC
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

%\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers
%\setstretch{1.5}
%% main text
\section{To Do List}
\begin{enumerate}
    \item Preliminary Results table for CCA, MCEM, MI
    \item Run Multiple imputation in R
    \item FamEvent updated version - simulation the family structure data
    \item Indexing
\end{enumerate}
\section{Weibull Parametric Approach \hl{(Discussions on why use parametric?)}}
From the beginning of the discussion, I have obtained the model, i.e., the hazard function is
\begin{equation}
    h_{ij}(t_{ij}|z_j)=h_0(t_{ij})\exp(\beta_1x_{1,ij}+\beta_2 x_{2,ij})z_j
\end{equation}
There are total $n_j$ individuals in family $j$, where $i=1,...,n_j$, and total $J$ families that $j=1,...,J$. $x_{1,ij}$ is the genotype, or say mutation gene status for individual $i$ in family $j$. $x_{2,ij}$
is the PRS for individual $i$ in family $j$. The frailty term $z_j$, has a pdf of $f(z)$, which can be Gamma, log-normal, or other common frailty distributions.
The support of $f(z)$ is always non-negative. The Weibull baseline hazard function is defined as
\begin{equation}
    h_0(t_{ij})=\alpha\lambda t_{ij}^{\lambda-1}
\end{equation}
where $\lambda$ is the shape parameter and $\alpha$ is the scale parameter. Let $\xi_{ij}=\exp(\beta_1 x_{1,ij}+\beta_2 x_{2,ij})$, the hazard function is 
\begin{equation}
    h_{ij}(t_{ij}|x_{ij}, g_{ij}, z_j)=\alpha\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j
\end{equation}
The survival function $S(t)$ can be obtained through cumulative hazard function $H(t)$
\begin{align}
    H(t_{ij}|x_{ij}, g_{ij}, z_j)&=\int_0^{t}h_{ij}(u|x_{ij}, g_{ij}, z_j)du\\
    &=\alpha\xi_{ij}z_j\lambda\int_0^t u^{\lambda-1}du\\
    &=\alpha\xi_{ij}z_j\lambda\cdot \frac{1}{\lambda} t_{ij}^{\lambda}=\alpha\xi_{ij}z_j t_{ij}^{\lambda}
\end{align}
and the survival function
\begin{equation}
    S(t_{ij}|x_{ij}, g_{ij}, z_j)=\exp(-H(t_{ij}|x_{ij}, g_{ij}, z_j))=\exp(-\alpha\xi_{ij}z_j t_{ij}^{\lambda})
\end{equation}
Let $\boldsymbol{\theta}=\{\beta_1, \beta_2, \alpha, \lambda, \boldsymbol{\phi}\}$, where $\boldsymbol{\phi}$ is the parameter vector for the frailty distribution of the choice. Therefore, the likelihood can be written as
\begin{align}
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\int_0^{\infty}\prod_{i=1}^{n_j}(\alpha\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha\xi_{ij}z_j t_{ij}^{\lambda})f(z)dz\\
    &=\prod_{j=1}^J\int_0^{\infty}\prod_{i=1}^{n_j}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))f(z)dz
\end{align}
So the log-likelihood is 
\begin{equation}\label{eq:logllhd}
    \ell(\boldsymbol{\theta})=\sum_{j=1}^J\log \left [ \int_0^{\infty}\prod_{i=1}^{n_j}h(t_{ij}|\mathbf{x}_{ij}, z_j)^{\delta_{ij}}\exp (-H(t_{ij}|\mathbf{x}_{ij}, z_j))f(z)dz\right ]
\end{equation}

\section{Gamma Frailty} 
The Laplace transform of the frailty $z\sim\text{Gamma}(k, k)$, for the simplicity of the mathematical expression, the following Laplace transform will ignore the subscript, denote $\mathscr{L}(f(z))=\phi(s)$ where $s=\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})$:
\begin{align}
    \phi(s)&=\int_0^{\infty}e^{-sz}f(z)dz\\
    &=\int_0^{\infty}e^{-sz}\frac{k^k}{\Gamma(v)}z^{k-1}e^{-kz}dz
\end{align}
Using the Gamma property: $\int_0^{\infty}z^{n-1}e^{-az}dz=\frac{\Gamma(n)}{a^n}$, $\phi(s)$ can be further written as
\begin{equation}
    \phi(s)=\frac{k^k}{\Gamma(k)}\int_0^{\infty}e^{-(s+k)z}z^{k-1}dz=\frac{k^k}{\Gamma(k)}\cdot \frac{\Gamma(k)}{(s+k)^k}=(1+\frac{s}{k})^{-k}
\end{equation}
The second derivative is $\frac{d^2\phi(s)}{ds^2}=\int_0^{\infty}(-z)^2e^{-sz}f(z)dz$. 

\noindent
The third derivative is $\frac{d^3\phi(s)}{ds^3}=\int_0^{\infty}(-z)^3e^{-sz}f(z)dz$, ...
Therefore, its $d$-th derivative, denote $\phi(s)^{(d)}$:
\begin{align}
    \phi(s)^{(d)}&=(-1)^d\int_0^{\infty}z^de^{-sz}f(z)dz\\
    &=(-1)^d\frac{(k+d-1)!}{(k-1)!(s+k)^d}(1+\frac{s}{k})^{-k}
\end{align}
Let $\boldsymbol{\theta}=(\beta_1, \beta_2, \alpha, \lambda, k)$ for Gamma frailty model, the log-likelihood is then written as
\begin{align}
    \ell(\boldsymbol{\theta})&=\sum_{j=1}^k\log \left [ \int_0^{\infty}\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}, z_j))^{\delta_{ij}}\exp (-H(t_{ij}|\mathbf{x}_{ij}, z_j))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\int_{0}^{\infty}\prod_{i=1}^{n_j}(z_j h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\exp(-z_j H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\int_0^{\infty}z_j^{d_j}\exp(-z_j\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j \right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\frac{(k+d_j-1)!}{(k-1)!(\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})+k)^{d_j}}\Big(1+\frac{\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})}{k}\Big)^{-k}\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}((h(t_{ij}|\mathbf{x}_{ij}) )^{\delta_{ij}})\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\log\left [h(t_{ij}|\mathbf{x}_{ij})^{\delta_{ij}} \frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\left [\sum_{i=1}^{n_j}(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij})) + \log\Big (\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j}\Big )\right ]
\end{align}
For each family $j$, the ascertainment $A_j$ is defined to be the probability of the proband $p$ being ascertained by the age $a_{j_p}$ at examination. Applying the ascertainment correction for the log-likelihood in family $j$: 
\begin{equation}
    \tilde{\ell}_{j}(\boldsymbol{\theta})=\ell_j(\boldsymbol{\theta})-\log A_j(\boldsymbol{\theta})
\end{equation}
where $\tilde{\ell}$ is the log-likelihood with ascertainment correction, and $\ell$ is the crude log-likelihood. Define $\mathbf{x}_{j_p}$ the covariate matrix for proband in family $j$. Note we can still apply Laplace transform here, such that
\begin{align}
    A_j(\boldsymbol{\theta})&=1-S_{j_p}(a_{j_p}|\mathbf{x}_{j_p})\\
    &=1-\int_0^{\infty} S_{j_p}(a_{j_p}|\mathbf{x}_{j_p},z_j)f(z_j)dz_j\\
    &=1-\int_0^{\infty}\exp(-z_j\cdot H_{j_p}(a_{j_p}|\mathbf{x}_{j_p}))f(z_j)dz_j\\
    &=1-(1+\frac{H_{j_p}(a_{j_p}|\mathbf{x}_{j_p})}{k})^{-k}
\end{align}
\section{Log-Normal Frailty}
The log-normal frailty is not the power-variance-function (PVF) family, so there is no closed form for Laplace transform or expressions for survivors. But we are able to estimate the Laplace transform using Gauss Hermite Quadrature. We typically standardize the log-normal frailty $Z$ as
\begin{align} 
    E(\log Z)&=0\\
    \text{Var}(\log Z)&=\sigma^2
\end{align} 
That is, $z\sim \text{log-Normal}(0, \sigma^2)$. The probability density function $f(z)$ is then
\begin{equation}\label{eq:lognormalfrailty}
    f(z)=\frac{1}{\sqrt{2\pi}\sigma}z^{-1}\exp (-\frac{\log (z)^2}{2\sigma^2})
\end{equation}
The Laplace transform is then
\begin{equation}
    \phi(s)=\mathscr{L}(f_Z)(s)=\int_0^{\infty}\exp(-sz)\cdot f(z)dz
\end{equation}
Using variable transformation, let $y=\frac{\log(z)}{\sqrt{2}\sigma}$, then $z=\exp(\sqrt{2}\sigma y)$, and $dz=\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy$. Therefore, for $d$-th derivative:
\begin{align}
    \phi(s)^d&=\int_{-\infty}^{\infty}z^d\exp(-sz)\cdot\frac{1}{\exp(\sqrt{2}\sigma y)\sigma\sqrt{2\pi}}\cdot\exp(-y^2)\cdot\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy\\
    &=\int_{-\infty}^{\infty}\exp(\sqrt{2}\sigma y)^d\exp(-s\exp(\sqrt{2}\sigma y))\cdot\frac{1}{\sqrt{\pi}}\exp(-y^2)dy
\end{align}
\begin{defn}[Gauss-Hermite Quadrature]\label{defn:gausshermite}
    The integrand part can be solved using Gauss-Hermite Quadrature. In numerical analysis, the method can be applied in the following form:
\begin{equation}
    \int_{-\infty}^{\infty}\exp(-x^2)f(x)dx\approx \sum_{i=1}^n\omega_i f(x_i)
\end{equation}
where $n$ is number of sample points used, and $x_i$ is the roots of Hermite polnomial $H_n (x)$ such that $i=1, ..., n$, and the weights $\omega_i$ is 
\begin{equation}
    \omega_i=\frac{2^{n-1}n!\sqrt{n}}{n^2[H_{n-1}(x_i)]^2}
\end{equation}
\end{defn}

\noindent
Applying Definition~\ref{defn:gausshermite}, the integral of the Laplace transform is then
\begin{equation}
    \phi(s)^d=\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\omega_{q}\exp(-s\exp(\sqrt{2}\sigma y_{q}))\exp(\sqrt{2}\sigma y_{q})^d
\end{equation}
where $q$ denotes the $q$-th element of Gauss Hermite Quadrature, i.e., $\omega_{q}$ denotes the $q$-th weight, $y_{q}$ denotes the $q$-th node, and $N_{q}$ denotes the total number of quadratures. Thus, substituting into the log-likelihood:
\begin{equation}
    \ell_j(\boldsymbol{\theta})=\sum_{i=1}^{n_j}\delta_{ij}\log(h(t_{ij}|\mathbf{x}_{ij}))+\log\Big (\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\left [\omega_{q}\exp(\sqrt{2}\sigma y_{q})^{d_j}\exp\Big (-\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})\exp(\sqrt{2}\sigma y_{q})\Big )\right ]\Big )
\end{equation}
Similarly, the ascertainment correction in the log-normal frailty can be written as 
\begin{align}
    A_j(\boldsymbol{\theta})&=1-\int_{-\infty}^{\infty} \exp(-z H(a_{j_p}|\mathbf{x}_{j_p}))f(z)dz\\
    &=1-\sum_{q=1}^{N_{q}}\omega_{q} \exp\left (-(\sum_{i=1}^{n_j} H(a_{j_p}|\mathbf{x}_{j_p}))\exp (\sqrt{2}\sigma y_{q_p})\right )
\end{align}

\section{Missing PRS and MCEM}\label{sec:missingprs}

\subsection{Not Considering the Family Correlations}
Given that family $j$ has some subjects containing the missing PRS due to the sampling cost (maybe), that not all subjects are being sampled for the PRS calculation. 
We propose a Monte Carlo sampling method within the MCEM framework in terms of estimating the distribution of the PRS. 
The PRS was calculated to infer the relationship between a phenotype and multiple genetic loci, while these information were not gained if one was not involved in the original GWAS. 
Thus, we propose to sample the PRS using the information that we have already obtained through the study. Denote $\mathbf{x}_{j,1}$ as the PRS scores vector in family $j$, and $\mathbf{x}_{j,2}$ the mutation status vector in family $j$. 
Take $\mathbf{p}_j$ as the proband indicator vector in family $j$, $\mathbf{c}_j$ is the current age for patients in family $j$. 
So we now have a design matrix when modelling the missing PRS, call it $\mathbf{W}=(\log(\mathbf{t}_j), \boldsymbol{\delta}_j, \log(\mathbf{t}_j)\odot \boldsymbol{\delta}_j, \mathbf{p}_j, \mathbf{c}_j, \mathbf{x}_{j,2})$.
We can make the assumption on the conditional distribution of the PRS, take $\mathbf{X}_{j,1}|\mathbf{W} \sim MVN (\mathbf{W}\boldsymbol{\psi}+\mathbf{u},\sigma^2\mathbf{I})$. 
We are interested in modelling the PRS while accounting for the between family variance, so $\mathbf{u}\sim MVN(0, \sigma_u^2\mathbf{I})$. Thus, the E-step for Gamma frailty model with ascertainment correction is then
\begin{align}
    E_{\mathbf{X}_{j,1,mis}}(\ell(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)}))&=\sum_{j=1}^J\Bigg [\sum_{i=1}^{n_j}\int_{\boldsymbol{X}_{j,1,mis}}(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}))+ \\ 
    & + \log\Big (\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j}\Big ) - \\
    & - \log(A_j(\boldsymbol{\theta})) + \log f(x_{ij,1,mis}|w_{ij},\boldsymbol{\psi}) dx_{ij,1,mis}\Bigg ]
\end{align}
Taking a sample of size $M$ when we sample $f(x_{ij,1,mis}|w_{ij}, \boldsymbol{\psi})$ for each subject $i$ in family $j$, $(x_{ij,i,mis}^{(1)}, ..., x_{ij,i,mis}^{(M)})$. 
This leads to an E-step:
\begin{align} 
    Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)})&=\sum_{j=1}^J\Bigg [\sum_{i=1}^{n_j}\sum_{m=1}^M(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}^{(m)}))+ \\ 
    & + \log\Big (\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}^{(m)}))}{k})^{-k-d_j}\Big ) - \\
    & - \log(A_j(\boldsymbol{\theta})) + \log f(x_{ij,1,mis}^{(m)}|w_{ij},\boldsymbol{\psi}) \Bigg ]
\end{align}
Similarly, the expectation with respect to the missing PRS in log-normal frailty can be written as
\begin{align} 
    E_{\mathbf{X}_{j,1,mis}}(\ell(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)}))&=\sum_{j=1}^J\Bigg [\sum_{i=1}^{n_j}\int_{\boldsymbol{X}_{j,1,mis}}(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij}))+ \\ 
    & + \log\Big (\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\Big [\omega_{q}\exp (\sqrt{2}\sigma y_{q})^{d_j}\exp (-\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})\exp(\sqrt{2}\sigma y_{q}))\Big]\Big ) - \\
    & - \log(A_j(\boldsymbol{\theta})) + \log f(x_{ij,1,mis}|w_{ij},\boldsymbol{\psi}) dx_{ij,1,mis}\Bigg ]
\end{align}
and the E-step:
\begin{align} 
    Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)}) &= \sum_{j=1}^J\Bigg [\sum_{i=1}^{n_j}\sum_{m=1}^{M}(\delta_{ij}\log h(t_{ij}|\mathbf{x}^{(m)}_{ij}))+ \\ 
    & + \log\Big (\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\Big [\omega_{q}\exp (\sqrt{2}\sigma y_{q})^{d_j}\exp (-\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}^{(m)}_{ij})\exp(\sqrt{2}\sigma y_{q}))\Big]\Big ) - \\
    & - \log(A_j(\boldsymbol{\theta})) + \log f(x_{ij,1,mis}^{(m)}|w_{ij},\boldsymbol{\psi})\Bigg ]
\end{align}
\subsection{Considering the Family Correlations}
Given that family $j$ has some subjects containing the missing PRS due to the sampling cost (maybe, need to confirm the previous paper), that not all subjects are being sampled for the PRS calculation. 
Since subjects within one family are correlated in some genetic associations, we intend to sample the missing PRS using a multivariate normal distribution while accounting for the kinship matrix. 
Using the same setting for the modelling of the PRS, the only difference is the variance-covariance matrix. 
Here, $\mathbf{X}_{j,1}|\mathbf{W},\boldsymbol{\psi}\sim MVN(\mathbf{W}\boldsymbol{\psi}+\mathbf{u}, \sigma^2\mathbf{I})$. 
But, $\mathbf{u}\sim MVN(0, 2\sigma^2_u\mathbf{K})$ where $\mathbf{K}$ is a $\mathbb{R}^{n\times n}$ kinship matrix with a diagonal of $0.5$. 
The variance can be computed via $\text{Var}(X_{j,1})=2\sigma_u^2\mathbf{K}+\sigma^2\mathbf{I}$ when both the genetic variation and the residual errors contribute to the variance-covariance structure. 
Therefore, the marginal distribution of the PRS after integrating out the random effects will be $\mathbf{X}_{j,1}\sim MVN(\mathbf{W}\boldsymbol{\psi}, 2\sigma_u^2\mathbf{K}+\sigma^2\mathbf{I})$, and will be the distribution used for the Monte Carlo sampling.
\hl{M Step, Nelder-Mead, why? Gradient free? Variance?}

\section{Multiple Imputation with Monte Carlo Sampling}
\subsection{Missing Mechanisms}
...A discussion on MCAR, MAR, and MNAR... 
\subsection{Multiple Imputations \hl{(Be sure to report a conceptual statistical definition in the thesis)}}
In the general framework, denote $\mathbf{X}_{j,1,mis}$ as the element of $\mathbf{X}_{j,1}$ when it's missing.
Denote $\mathbf{X}_{j,1,obs}$ on the observed element. 
Adapting the idea from the MCEM, we can draw $\hat{\mathbf{X}}_{j,1,mis}$ from the same assumption of the distribution discussed in the section~\ref{sec:missingprs} for $M$ times.
But, the missing indicator $\mathbf{R}$ should be addressed to attest the missing mechanism. 
The conditional distribution can be written as $f(\mathbf{X}_{j,1,mis}|\mathbf{X}_{j,1,obs}, \mathbf{W}, \mathbf{R}_j)$. 
Under the assumption of the MAR \hl{(Confirm the paper)}, 
\begin{equation}
    f(\mathbf{X}_{j,1,mis}|\mathbf{X}_{j,1,obs}, \mathbf{W}, \mathbf{R}_j)=f(\mathbf{X}_{j,1,mis}|\mathbf{X}_{j,1,obs}, \mathbf{W})
\end{equation}
This results a series of $M$ complete datasets containing all values observed with the fill of the Monte Carlo samples. 
Then with these $M$ datasets, we run the analysis individually. 
Thus, the estimate of $\boldsymbol{\theta}$ can be calculated via 
\begin{equation}
    \hat{\boldsymbol{\theta}}_{MI}=\frac{1}{M}\sum_{m=1}^M\hat{\boldsymbol{\theta}}_m
\end{equation}
Denote $V$ the variance for $m$-th complete data inference variance, we first calculate the average variance of these analyses to be the within-imputation variance,
\begin{equation}
    \hat{\mathbf{W}}=\frac{1}{M}\sum_{m=1}^M\hat{\mathbf{V}}_m 
\end{equation}
Under the unbiased variance estimator, the between-imputation variance is
\begin{equation}
    \hat{\mathbf{B}}=\frac{1}{M-1}\sum_{m=1}^M (\hat{\boldsymbol{\theta}}_m - \hat{\boldsymbol{\theta}}_{MI})(\hat{\boldsymbol{\theta}}_m - \hat{\boldsymbol{\theta}}_{MI})^{\top}
\end{equation}
Combining these \hl{(How? Are there any statistical/mathematical proof of this)}, we obtain the estimate of the variance of $\hat{\boldsymbol{\theta}}_{MI}$,
\begin{equation}
    \hat{\mathbf{V}}_{MI}=\hat{\mathbf{W}}+(1+\frac{1}{M})\hat{\mathbf{B}}
\end{equation}

\hl{Discussions on why MI over MCEM or vise versa after the analysis...}

\section{Monte Carlo EM \hl{(This section needs to be re-written to a definitional MCEM framework, maybe consult papers)}}
The complete data log-likelihood for family $j$ is $\ell_j(\boldsymbol{\theta}; h_{ij})$
where $\boldsymbol{\theta}$ consists all baseline parameters, and model coefficients $\beta$'s, as well as the frailty parameter $\phi$. The E-step for complete data is:
\begin{align} 
    Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(r)})=\int \ell(\boldsymbol{\theta};h_{ij})\cdot f(x_{mis,i}|x_{obs,i},z, \boldsymbol{\theta}^{(r)}, t_{ij}, \delta_{ij}, p_j)dx_{mis,ij}
\end{align}

We sample the size $m_i$ for each $i$-th observation, $x_{i1}^*,...,x_{im_i}^*$ from the distribution $f(x_{mis,ij}|\cdot)$, and take $M = 1,...,m_i$, such that each $X_{iM}^*$ depends on the iteration number for $r+1$ iterations. In general:
\begin{align}
    \hat{Q}(\boldsymbol{\theta}; \boldsymbol{\theta}^{(r)})=\frac{1}{m_i}\sum_{M=1}^{m_i}\ell(x_{iM}^*, x_{obs,ij},t_{ij}, \boldsymbol{\theta},z_j)
\end{align}
More specifically, 
\begin{enumerate}
    \item We first initialize $m, \theta^{(0)}$, and start the burn-in.
    \item Also, we set importance weights $w_t=1$ for all $t=1,...,m$.
    \item At the burn-in iteration $s$, we generate $x_{miss,1}, ..., x_{miss,m}\sim N(\mu_{X}|X_{obs}, \theta^{(s)}, z)$ using MCMC sample.
    \item In the E-step, we estimate $Q(\theta|\theta^{(s)})$ by using the importance weights:
    \begin{equation}
        Q_m(\theta|\hat{\theta}^{(s)})=\frac{\sum_{t=1}^m w_t\log f(X_{obs}, X_{miss,t}|\theta)}{\sum_{r=1}^mw_t}
    \end{equation}
    \item Note the numerator is actually a weighted log-likelihood. In the M-step, we maximize $Q_m(\theta|\hat{\theta}^{(s)})$ to obtain $\hat{\theta}^{(s+1)}$. 
    \item Repeat (3.) - (5.) for $s$ burn-in iterations.
    \item Then re-initialize $\hat{\theta}^{(0)}=\hat{\theta}^{(s)}$
    \item We generate $x_{miss,1},...,x_{miss,m}\sim N(\mu_{X}|X_{obs}, \hat{\theta}^{(0)}, z)$ using MCMC sampler. At iteration $r+1$
    \item Compute the importance weights from the ratio of likelihood
    \begin{equation}
        w_t=\frac{L(\hat{\theta}^{(r)}|X_{miss,t}, X_{obs})}{L(\hat{\theta}^{(0)}|X_{miss,t}, X_{obs})}
    \end{equation}
    \item Thus, the E-step can be written as 
    \begin{equation}
        Q_m(\theta|\hat{\theta}^{(r)})=\frac{\sum_{t=1}^mw_t\log f(X_{miss,t}, X_{obs}|\theta)}{\sum_{t=1}^m w_t}
    \end{equation}
    \item Then M-step: we maximize $Q_m(\theta|\hat{\theta}^{(r)})$ to obtain $\hat{\theta}^{(r+1)}$. 
\end{enumerate}
This automated MCEM firstly optimizes the importance weights at burn-ins, then performs the actual EM to find $\hat{\theta}$. This importance weight ensures the imputation step of the missing data actually yields to the real distribution. 
   


\section{Correlated Frailty using Kinship Matrix}
Family members are correlated within one family, that we denote $K$ as the kinship correlation matrix among all observations. This matrix ensures those individuals not from the same family automatically have a correlation of 0. The likelihood construction needs multivariate form. For $\mathbf{Z}\sim\text{MVN}(0,\sigma^2K)$, that $K$ has the diagonal of 1. The likelihood is 

\begin{align}
    L(\cdot)&=\int_{\mathbb{R}^n}\prod_{i=1}^n(h(t|\mathbf{x}_i, \mathbf{z}_i))^{\delta_i}\exp (-H(t|\mathbf{x}_i, \mathbf{z}_i))f(\mathbf{z})d\mathbf{z}\\
    &=\int_{\mathbb{R}^n}\prod_{i=1}^n(h(t|\mathbf{x}_i))^{\delta_i}\exp(\mathbf{z}_i)^{\delta_i}\exp(-H(t|\mathbf{x}_i)\exp(\mathbf{z}_i))f(\mathbf{z})d\mathbf{z}\\
    &=\prod_{i=1}^n(h(t|\mathbf{x}_i))^{\delta_i}\int_{\mathbb{R}^n}\exp(\delta_i\mathbf{z}_i-H(t|\mathbf{x}_i)\exp(\mathbf{z}_i))f(\mathbf{z})d\mathbf{z}
\end{align}
Applying the Laplace approximation, and taking the log for the likelihood, we obtain
\begin{equation}
    \ell(\cdot)=\sum_{i=1}^n\Big [ \delta_i\log h(t|\mathbf{x}_i)\Big ] + \sum_{i=1}^n\Big [\delta_i \hat{\mathbf{z}}-H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})\Big ] - \frac{1}{2}\hat{\mathbf{z}}^{\top}\Sigma^{-1}\hat{\mathbf{z}}
\end{equation}
such that $\Sigma=\sigma^2 K$. Also, we treat the random effect $\mathbf{z}$ as a vector of parameters, and use outer-loop to search for the $\sigma$, and use inner-loop to search for other parameters (baseline parameters, and $\beta$) including $\mathbf{z}$. The process can be achieved via Newton-Raphson algorithm. For computational efficiency, we can set $\Sigma^{-1}=L^{\top}L$ through Cholesky Decomposition. In this way, 
$\mathbf{z}L\sim MVN(0,\sigma^2 I)$. In order to apply NR-algorithm, the gradient and the hessian are required. The gradient for parameters is:
\begin{equation}
    \frac{\partial\ell}{\partial\boldsymbol{\beta}}=\sum_{i=1}^n\delta_i\mathbf{x}_i+\sum_{i=1}^n-H(t_i|\mathbf{x}_i)\mathbf{x}_i\exp(\mathbf{z})
\end{equation}

\begin{equation}
    \frac{\partial\ell}{\partial\mathbf{z}}=\sum_{i=1}^n\delta_i-(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})-\Sigma^{-1}\hat{\mathbf{z}}
\end{equation}

\begin{equation}
    \frac{\partial\ell}{\partial\alpha}=\sum_{i=1}^n\frac{\delta_i}{\alpha} + \sum_{i=1}^n-\frac{H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})}{\alpha}
\end{equation}

\begin{equation}
    \frac{\partial\ell}{\partial\lambda}=\sum_{i=1}^n\delta_i(\frac{1}{\lambda}+\log (t_i))+\sum_{i=1}^n-H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})\log (t_i)
\end{equation}

The hessian matrix element, i.e., second partial derivative is 

\begin{equation}
    \frac{\partial^2\ell}{\partial\boldsymbol{\beta}^{\top}\boldsymbol{\beta}}=\sum_{i=1}^n-H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})x_{ij}x_{ik}
\end{equation}

\begin{equation}
    \frac{\partial^2\ell}{\partial\mathbf{z}^{\top}\mathbf{z}}=\sum_{i=1}^n-H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})-\Sigma^{-1}
\end{equation}

\begin{equation}
    \frac{\partial^2\ell}{\partial\alpha^2}=\sum_{i=1}^n-\frac{\delta_i}{\alpha^2}
\end{equation}

\begin{equation}
    \frac{\partial^2\ell}{\partial\lambda^2}=\sum_{i=1}^n-\frac{\delta_i}{\lambda^2}-H(t_i|\mathbf{x}_i)\exp(\hat{\mathbf{z}})\log(t_i)^2
\end{equation}

\subsection{Proof of $\Sigma=LL^{\top}$}
Every symmetric positive definite matrix $\Sigma$ can be decomposed into $\Sigma=LL^{\top}$, where $L$ is a lower triangular matrix with real and positive diagonal entries. 
\begin{proof}
    Set-ups:
\begin{enumerate}
    \item Covariance matrix $\boldsymbol{\Sigma}$ is by definition symmetric and positive definite, e.g.
    \begin{equation}
        \boldsymbol{\Sigma}=\begin{pmatrix}
            \sigma_{X_1}^2 & Cov(X_1,X_2)\\
            Cov(X_1,X_2) & \sigma_{X_2}^2
        \end{pmatrix}
    \end{equation}
    such that $\mathbf{X}\boldsymbol{\Sigma} \mathbf{X}^{\top}>0$ always, and this matrix is symmetric.
    \item Suppose $\mathbf{X}$ has $n$ observations, then $\Sigma$ is $n\times n$, the first element is $\sigma_{11}>0$ by definition (For simplicity, we use $\sigma_{11}$ rather than it's square to denote the variance). Define $l_{11}=\sqrt{\sigma_{11}}$, to be the first element of $L$. For the first column of $L$, let $l_{j1}=\frac{\sigma_{j1}}{l_{11}}$ for $j=2,...$.
\end{enumerate}
Induction step: Assume we have first $k-1$ columns of $L$, consider $k$-th column
\begin{itemize}
    \item For the diagonal element $l_{kk}=\sqrt{\sigma_{kk}-\sum_{j=1}^{k-1}l_{kj}^2}$
    \item For off-diagonals, 
    \begin{equation}
        l_{ik}=\frac{\sigma_{ik}-\sum_{j=1}^{k-1}l_{ij}l_{kj}}{l_{kk}}
    \end{equation}
    for $i=k+1,...,n$.
\end{itemize}
with the repetition for each column $k=2,...,n$, the top-left $k\times k$ submatrix of $LL^{\top}$ matches that of $\boldsymbol{\Sigma}$. For example, when $k=3$, 
\begin{equation}
    \boldsymbol{\Sigma}=
        \begin{pmatrix}
            \sigma_{11} & & \\
             & \sigma_{22} & \\
             & & \sigma_{33}
        \end{pmatrix}
\end{equation}
and
\begin{equation}
    L = 
    \begin{pmatrix}
        l_{11} & 0 & 0\\
        l_{21} & l_{22} & 0 \\
        l_{31} & l_{32} & l_{33}
    \end{pmatrix}
\end{equation}
then
\begin{equation}
    LL^{\top}=
    \begin{pmatrix}
        l_{11} & 0 & 0\\
        l_{21} & l_{22} & 0 \\
        l_{31} & l_{32} & l_{33}
    \end{pmatrix}
    \begin{pmatrix}
        l_{11} & l_{21} & l_{31}\\
        0 & l_{22} & l_{32} \\
        0 & 0 & l_{33}
    \end{pmatrix}
    =
    \begin{pmatrix}
        l_{11}^2 & l_{11}l_{21} & l_{11}l_{31} \\ 
        l_{21}l_{11} & l_{21}^2+l_{22}^2 & l_{21}l_{31} + l_{22}l_{32} \\
        l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^2+l_{32}^2+l_{33}^2
    \end{pmatrix}
\end{equation}
Take 
\begin{equation}
    \Sigma=
    \begin{pmatrix}
        4 & 2 & 2\\
        2 & 3 & 1\\
        2 & 1 & 3
    \end{pmatrix}
\end{equation}
Then by definition of Cholesky Decomposition, we can calculate $l_{11}^2=\sigma_{11}\implies l_{11}=\sqrt{4}=2$, and $l_{21}=\frac{\sigma_{21}}{l_{11}}=2/2=1$, and $l_{31}=1$. Similarly for $l_{22}, l_{32}, l_{33}$. Therefore, 
\begin{equation}
    L=
    \begin{pmatrix}
        2 & 0 & 0 \\
        1 & \sqrt{2} & 0\\
        1 & 0 & \sqrt{2}
    \end{pmatrix}
\end{equation}
which implies
\begin{equation}
    LL^{\top}=
    \begin{pmatrix}
        2 & 0 & 0 \\
        1 & \sqrt{2} & 0\\
        1 & 0 & \sqrt{2}
    \end{pmatrix}
    \begin{pmatrix}
        2 & 1 & 1 \\
        0 & \sqrt{2} & 0 \\
        0 & 0 & \sqrt{2}
    \end{pmatrix}
    =
    \begin{pmatrix}
        4 & 2 & 2\\
        2 & 3 & 1\\
        2 & 1 & 3
    \end{pmatrix}
    =\Sigma
\end{equation}
\end{proof}
Essentially, the Cholesky Decomposition transforms the multivariate normal to a standard multivariate normal. When $\mathbf{Z}\sim \mathcal{N}(0,\boldsymbol{\Sigma})$, let $\boldsymbol{\Sigma}=\mathbf{L}\mathbf{L}^{\top}$, then $\mathbf{Y}=\mathbf{L}^{-1}\mathbf{Z}\sim \mathcal{N}(0, \mathbf{I})$ that $\mathbf{I}$ is the identity matrix, since $\mathbf{L}^{-1}\boldsymbol{\Sigma}(\mathbf{L}^{-1})^{\top}=\mathbf{L}^{-1}\mathbf{L}\mathbf{L}^{\top}(\mathbf{L}^{-1})^{\top}=\mathbf{I}$. This will simplify the computational process. 





%\section{Stochastic EM?}




%\newpage

%\section{References}
%\label{S.7}
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

%\bibliographystyle{model1-num-names}
%\bibliography{sample.bib}
%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

%\begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

%\bibitem{}

%\end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.