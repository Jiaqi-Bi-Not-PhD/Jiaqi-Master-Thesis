\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{setspace}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage[scr]{rsfso}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{natbib}
%\usepackage{glossaries}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\usepackage[symbols,nogroupskip,sort=none]{glossaries-extra}

\glsxtrnewsymbol[description={Individual index}]{i}{\ensuremath{i}}
\glsxtrnewsymbol[description={Family (Cluster) index}]{j}{\ensuremath{j}}
\glsxtrnewsymbol[description={Proband index}]{p}{\ensuremath{p}}
\glsxtrnewsymbol[description={Number of events in family $j$}]{dj}{$d_j$}
\glsxtrnewsymbol[description={Some time}]{t}{\ensuremath{t}}
\glsxtrnewsymbol[description={Some Time for the proband}]{a}{\ensuremath{a}}
\glsxtrnewsymbol[description={Event Time}]{T}{\ensuremath{T}}
\glsxtrnewsymbol[description={Event indicator for individual $i$ in family $j$}]{delta}{$\delta_{ij}$}
\glsxtrnewsymbol[description={The observed survival data ($t,\delta$)}]{w}{\ensuremath{w}}
\glsxtrnewsymbol[description={Number of individuals}]{n}{\ensuremath{n}}
\glsxtrnewsymbol[description={Number of Families (Clusters)}]{J}{\ensuremath{J}}
\glsxtrnewsymbol[description={Index of the sampled completed dataset in the MCEM}]{m}{\ensuremath{m}}
\glsxtrnewsymbol[description={Number of the sampled completed dataset in the MCEM}]{M}{\ensuremath{M}}
\glsxtrnewsymbol[description={Frailty term}]{z}{\ensuremath{z}}
\glsxtrnewsymbol[description={$q$-th element of Gauss Hermite Quadrature}]{q}{\ensuremath{q}}
\glsxtrnewsymbol[description={$q$-th weight of Gauss Hermite Quadrature}]{omega}{\ensuremath{\omega}}
\glsxtrnewsymbol[description={$q$-th node of Gauss Hermite Quadrature}]{yq}{\ensuremath{y_q}}
\glsxtrnewsymbol[description={Total number of quadratures}]{Nq}{\ensuremath{N_q}}
\glsxtrnewsymbol[description={Hazard fucntion}]{h}{\ensuremath{h(\cdot)}}
\glsxtrnewsymbol[description={Baseline hazard function}]{h_0}{\ensuremath{h_0(\cdot)}}
\glsxtrnewsymbol[description={Cumulative hazard fucntion}]{H}{\ensuremath{H(\cdot)}}
\glsxtrnewsymbol[description={Survival fucntion}]{S}{\ensuremath{S(\cdot)}}
\glsxtrnewsymbol[description={Ascertainment of family $j$ into the study}]{A}{$A_j(\cdot)$}
\glsxtrnewsymbol[description={Likelihood function}]{L}{$L(\cdot)$}
\glsxtrnewsymbol[description={Log-likelihood function}]{ell}{$\ell(\cdot)$}
\glsxtrnewsymbol[description={Laplace transform}]{lap}{$\mathscr{L}(\cdot)$}
\glsxtrnewsymbol[description={Covariates}]{X}{\ensuremath{\mathbf{x}}}
\glsxtrnewsymbol[description={Model coefficients vector}]{beta}{$\boldsymbol{\beta}$}
\glsxtrnewsymbol[description={Parameter vector}]{theta}{$\boldsymbol{\theta}$}
\glsxtrnewsymbol[description={The combination of $(\boldsymbol{\beta}, \lambda, \alpha)$}]{Lambda}{$\Lambda$}
\glsxtrnewsymbol[description={Weibull shape parameter}]{lambda}{$\lambda$}
\glsxtrnewsymbol[description={Weibull scale parameter}]{alpha}{$\alpha$}
\glsxtrnewsymbol[description={General form of the parameter in an undefined frailty distribution}]{upsilon}{$\upsilon$}
\glsxtrnewsymbol[description={Gamma shape and rate parameters}]{k}{$k$}
\glsxtrnewsymbol[description={Log-Normal variance parameter}]{sigma}{$\sigma^2$}
\glsxtrnewsymbol[description={Missing data distribution parameters}]{psi}{$\psi$}








\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argmaxA}{arg\,max}
\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

%%% Change the date
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\renewcommand*{\today}{Mar. 29, 2024}

\journal{Dr. Choi \& Dr. Espin-Garcia}



\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Jiaqi's Thesis Progress Report}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}


\author[rvt]{Jiaqi Bi}
\ead{jbi23@uwo.ca}
%\author[rvt]{Jiaqi Bi\corref{cor1}\fnref{fn1}}
%\ead{jbi23@uwo.ca}
%\author[rvt]{E.~Deccache\fnref{fn1}}
%\ead{elciodeccache@unifei.edu.br }
%\author[rvt]{B.D.~Bonatto\fnref{fn1}}
%\ead{bonatto@unifei.edu.br }
%\author[rvt]{H.~Arango\fnref{fn1}}
%\ead{hector.arango@uol.com.br }
%\author[rvt]{E.O.~Pamplona\fnref{fn2}}
%\ead{pamplona@unifei.edu.br}
%%\author[els]{E.O.~Pamplona\corref{cor2}\fnref{fn1,fn3}}
%%\ead[url]{pamplona@unifei.edu.br}
%%\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Affliated with Schulich School of Medicine \& Dentistry, Dept. Epidemiology and Biostatistics}
%\fntext[fn1]{Study Affiliated with Department of Statistical and Actuarial Sciences}
%\cortext[cor2]{Principal corresponding author}
%\fntext[fn1]{CERIn - Center of Excellence in Smart Grids}
%\fntext[fn2]{IEPG - Management \& Production Engineering Institute \\Unifei - Federal University of Itajuba}

\address[rvt]{Western University, \\ Schulich School of Medicine \& Dentistry, \\ Department of Epidemiology and Biostatistics}

%\tnotetext[t1]{The authors would like to thank ELETROBRAS, ANEEL, INERGE, CAPES, CNPq and FAPEMIG for financially supporting this research.}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

%\begin{abstract}
%\setstretch{1.2}
%% Text of abstract

%\end{abstract}

%\begin{keyword}
%Electricity theft \sep Regulated Electricity Company \sep Economic Impact \sep Tarot \sep Operational Optimal Point.
%% keywords here, in the form: keyword \sep keyword
%Missing data \sep Multiple imputation \sep complete case analysis \sep EM algorithm \sep MCMC
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

%\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers
%\setstretch{1.5}
%% main text
\section{To Do List}
\begin{enumerate}
    \item Simulation Study
\end{enumerate}
\section{Notations}
\printunsrtglossary[type=symbols,style=long, title = {List of Notations}]

\section{Frailty Model with Weibull Baseline Hazard}
For the model efficiency of the analyses in a genetic research, a parametric survival analysis is usually chosen over semi-parametric survival analysis~\cite{rudolph2018parametric, berger2001statistical}.
From the beginning of the discussion, I have obtained the model, i.e., the hazard function is
\begin{equation}
    h_{ij}(t_{ij}|\mathbf{x}_{ij}, z_j)=h_0(t_{ij})\exp(\beta_1x_{1,ij}+\beta_2 x_{2,ij})z_j
\end{equation}
There are total $n_j$ individuals in family $j$, where $i=1,...,n_j$, and total $J$ families that $j=1,...,J$. $x_{1,ij}$ is the genotype, or say mutation gene status for individual $i$ in family $j$. $x_{2,ij}$
is the PRS for individual $i$ in family $j$. The frailty term $z_j$, has a pdf of $f(z)$, which can be Gamma, log-normal, or other frailty distributions.
The support of $f(z)$ is always non-negative. The Weibull baseline hazard function is defined as
\begin{equation}
    h_0(t_{ij})=\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}
\end{equation}
where $\lambda$ is the shape parameter and $\alpha$ is the rate parameter. Let $\xi_{ij}=\exp(\beta_1 x_{1,ij}+\beta_2 x_{2,ij})$, the hazard function is 
\begin{equation}
    h_{ij}(t_{ij}|\mathbf{x}_{ij}, z_j)=\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j
\end{equation}
The survival function $S(t)$ can be obtained through cumulative hazard function $H(t)$
\begin{align}
    H(t_{ij}|\mathbf{x}_{ij}, z_j)&=\int_0^{t}h_{ij}(u|\mathbf{x}_{ij}, z_j)du\\
    &=\alpha^{\lambda}\xi_{ij}z_j\lambda\int_0^t u^{\lambda-1}du\\
    &=\alpha^{\lambda}\xi_{ij}z_j\lambda\cdot \frac{1}{\lambda} t_{ij}^{\lambda}=\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda}
\end{align}
and the survival function
\begin{equation}
    S(t_{ij}|\mathbf{x}_{ij}, z_j)=\exp(-H(t_{ij}|\mathbf{x}_{ij}, z_j))=\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})
\end{equation}
Let $\boldsymbol{\theta}=\{\boldsymbol{\beta}, \alpha, \lambda, \upsilon\}$, where $\upsilon$ is the parameter for the frailty distribution of the choice. 
In our example dataset, $\boldsymbol{\beta}=(\beta_1, \beta_2)$. 
Therefore, the likelihood assuming missing data and frailties are observed can be written as
\begin{align}
    L(\boldsymbol{\theta}|z_j)&=\prod_{j=1}^J\prod_{i=1}^{n_j}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))
\end{align}
When there is no missing data but frailties are present, the frailty term can be integrated where the likelihood is taken to be the expectation with respect to the frailty $z_j$. 
The likelihood can be written as 
\begin{align}
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})f(z_j)dz_j\label{eq:frailtyonlyllhd1}\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))f(z_j)dz_j\label{eq:frailtyonlyllhd2}
\end{align}
But when the missing data and the frailty both exist in the model, we will need to account for their joint distribution within the likelihood according to~\citet{herring2002frailty}. 
\begin{align} 
    L(\boldsymbol{\theta})&=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j,\mathbf{x}_{mis,ij}}(\alpha^{\lambda}\lambda t_{ij}^{\lambda-1}\xi_{ij}z_j)^{\delta_{ij}}\exp(-\alpha^{\lambda}\xi_{ij}z_j t_{ij}^{\lambda})f(z_j, \mathbf{x}_{mis,ij})dz_jd\mathbf{x}_{mis,ij}\label{eq:frailtyandmissing1}\\
    &=\prod_{j=1}^J\prod_{i=1}^{n_j}\int_{z_j,\mathbf{x}_{mis,ij}}h(t_{ij}|\mathbf{x}_{ij},z_j)^{\delta_{ij}}\exp(-H(t_{ij}|\mathbf{x}_{ij},z_j))f(z_j, \mathbf{x}_{mis,ij})dz_jd\mathbf{x}_{mis,ij}\label{eq:frailtyandmissing2}
\end{align}
The following section~\ref{sec:gammafr} and section~\ref{sec:lognormfr} discuss how to handle the frailty within the likelihood when there are no missing data, which are corresponding to the Equation~\ref{eq:frailtyonlyllhd1} and~\ref{eq:frailtyonlyllhd2}. 
The section~\ref{sec:likeandmissing} will discuss how to handle the frailty and the missing data jointly, which is corresponding to the likelihood equation~\label{eq:frailtyandmissing1} and~\ref{eq:frailtyandmissing2}.

\section{Ascertainment Correction} 
Within a genetic study, those families are typically selected when there is an affected person called a proband. This will yield a selection bias because this is no long a case-control study, and can potentially defect the statistical power~\cite{park2015adjusting, clark2005ascertainment}.
It is crucial to address the ascertainment bias. Consider $A$ as the event of being ascertained, $D$ as the data, we then have $P(D, A|\boldsymbol{\theta})=P(A|D,\boldsymbol{\theta})P(D|\boldsymbol{\theta})$. 
Also, we know $A$ is included in $D$, from Baye's rule
\begin{equation} 
    P(D|\boldsymbol{\theta})= \frac{P(D,A|\boldsymbol{\theta})}{P(A|D, \boldsymbol{\theta})}\propto\frac{L(\boldsymbol{\theta}|D)}{P(A|D,\boldsymbol{\theta})}
\end{equation}
For each family $j$, the ascertainment $A_j$ is defined to be the probability of the proband $p$ being ascertained by the age $a_{p_j}$ at examination, i.e., $A_j=P(T_{p_j} < a_{p_j})$ where $a_{p_j}$ is proband's age at study entry. Applying the ascertainment correction for the log-likelihood in family $j$: 
\begin{equation}
    \tilde{\ell}_{j}(\boldsymbol{\theta})=\ell_j(\boldsymbol{\theta})-\log A_j(\boldsymbol{\theta})
\end{equation}
where $\tilde{\ell}$ is the log-likelihood with ascertainment correction, and $\ell$ is the crude log-likelihood. 
Define $\mathbf{x}_{p_j}$ the covariates for proband in family $j$, so we can further write the formula for the ascertainment correction within different frailty models. 

\section{Gamma Frailty}\label{sec:gammafr}
We can obtain the likelihood for Gamma frailty model following the instruction by~\citet{balan2020tutorial}. 
The Laplace transform of the frailty $z\sim\text{Gamma}(k, k)$, for the simplicity of the mathematical expression, the following Laplace transform will ignore the subscript, denote $\mathscr{L}(f(z))=\phi(s)$ where $s=\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})$:
\begin{align}
    \phi(s)&=\int_0^{\infty}e^{-sz}f(z)dz\\
    &=\int_0^{\infty}e^{-sz}\frac{k^k}{\Gamma(v)}z^{k-1}e^{-kz}dz
\end{align}
Using the Gamma property: $\int_0^{\infty}z^{n-1}e^{-az}dz=\frac{\Gamma(n)}{a^n}$, $\phi(s)$ can be further written as
\begin{equation}
    \phi(s)=\frac{k^k}{\Gamma(k)}\int_0^{\infty}e^{-(s+k)z}z^{k-1}dz=\frac{k^k}{\Gamma(k)}\cdot \frac{\Gamma(k)}{(s+k)^k}=(1+\frac{s}{k})^{-k}
\end{equation}
The second derivative is $\frac{d^2\phi(s)}{ds^2}=\int_0^{\infty}(-z)^2e^{-sz}f(z)dz$. 

\noindent
The third derivative is $\frac{d^3\phi(s)}{ds^3}=\int_0^{\infty}(-z)^3e^{-sz}f(z)dz$, ...
Therefore, its $d$-th derivative, denote $\phi(s)^{(d)}$:
\begin{align}
    \phi(s)^{(d)}&=(-1)^d\int_0^{\infty}z^de^{-sz}f(z)dz\\
    &=(-1)^d\frac{(k+d-1)!}{(k-1)!(s+k)^d}(1+\frac{s}{k})^{-k}
\end{align}
Let $\boldsymbol{\theta}=(\beta_1, \beta_2, \alpha, \lambda, k)$ for Gamma frailty model, the log-likelihood is then written as
\begin{align}
    \ell(\boldsymbol{\theta})&=\sum_{j=1}^k\log \left [ \int_0^{\infty}\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}, z_j))^{\delta_{ij}}\exp (-H(t_{ij}|\mathbf{x}_{ij}, z_j))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\int_{0}^{\infty}\prod_{i=1}^{n_j}(z_j h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\exp(-z_j H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\int_0^{\infty}z_j^{d_j}\exp(-z_j\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij}))f(z_j)dz_j \right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}(h(t_{ij}|\mathbf{x}_{ij}))^{\delta_{ij}}\frac{(k+d_j-1)!}{(k-1)!(\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})+k)^{d_j}}\Big(1+\frac{\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})}{k}\Big)^{-k}\right ]\\
    &=\sum_{j=1}^J\log\left [\prod_{i=1}^{n_j}((h(t_{ij}|\mathbf{x}_{ij}) )^{\delta_{ij}})\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\log\left [h(t_{ij}|\mathbf{x}_{ij})^{\delta_{ij}} \frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j} \right ]\\
    &=\sum_{j=1}^J\left [\sum_{i=1}^{n_j}(\delta_{ij}\log h(t_{ij}|\mathbf{x}_{ij})) + \log\Big (\frac{(k+d_j-1)!}{k!k^{d_j-1}}(1+\frac{\sum_{i=1}^{n_j}(H(t_{ij}|\mathbf{x}_{ij}))}{k})^{-k-d_j}\Big )\right ]
\end{align}
 Not all probands in their study entry ages are affected, so it is crucial to apply a ascertainment correction accordingly. 
 Denote $I(T_{p_j}<a_{p_j})$ as an indicator of the proband was affected before their entry to the study.
 Note we can still apply Laplace transform for the ascertainment correction, such that
\begin{align}
    A_j(\boldsymbol{\theta})&=\Big [1-S_{p_j}(a_{p_j}|\mathbf{x}_{p_j})\Big ]^{I(T_{p_j}<a_{p_j})}S_{p_j}\Big [(a_{p_j}|\mathbf{x}_{p_j})\Big ]^{1-I(T_{p_j}<a_{p_j})}\\
    &=\Big [1-\int_0^{\infty} S_{p_j}(a_{p_j}|\mathbf{x}_{p_j},z_j)f(z_j)dz_j\Big ]^{I(T_{p_j}<a_{p_j})}\Big [\int_0^{\infty} S_{p_j}(a_{p_j}|\mathbf{x}_{p_j},z_j)f(z_j)dz_j\Big ]^{1-I(T_{p_j}<a_{p_j})}\\
    &=\Big [1-\int_0^{\infty}\exp(-z_j\cdot H_{p_j}(a_{p_j}|\mathbf{x}_{p_j}))f(z_j)dz_j\Big ]^{I(T_{p_j}<a_{p_j})} \\
    &\times \Big [ \int_0^{\infty}\exp(-z_j\cdot H_{p_j}(a_{p_j}|\mathbf{x}_{p_j}))f(z_j)dz_j\Big ]^{1-I(T_{p_j}<a_{p_j})}\\
    &=\Big [1-(1+\frac{H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})}{k})^{-k}\Big ]^{I(T_{p_j}<a_{p_j})}\Big [ (1+\frac{H_{p_j}(a_{p_j}|\mathbf{x}_{p_j})}{k})^{-k}\Big ]^{1-I(T_{p_j}<a_{p_j})}
\end{align}
\section{Log-Normal Frailty}\label{sec:lognormfr}
The log-normal frailty is not the power-variance-function (PVF) family, so there is no closed form for Laplace transform or expressions for survivors. But we are able to estimate the Laplace transform using Gauss Hermite Quadrature. We typically standardize the log-normal frailty $Z$ as
\begin{align} 
    E(\log Z)&=0\\
    \text{Var}(\log Z)&=\sigma^2
\end{align} 
That is, $z\sim \text{log-Normal}(0, \sigma^2)$. The probability density function $f(z)$ is then
\begin{equation}\label{eq:lognormalfrailty}
    f(z)=\frac{1}{\sqrt{2\pi}\sigma}z^{-1}\exp (-\frac{\log (z)^2}{2\sigma^2})
\end{equation}
The Laplace transform is then
\begin{equation}
    \phi(s)=\mathscr{L}(f_Z)(s)=\int_0^{\infty}\exp(-sz)\cdot f(z)dz
\end{equation}
Using variable transformation, let $y=\frac{\log(z)}{\sqrt{2}\sigma}$, then $z=\exp(\sqrt{2}\sigma y)$, and $dz=\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy$. Therefore, for $d$-th derivative:
\begin{align}
    \phi(s)^d&=\int_{-\infty}^{\infty}z^d\exp(-sz)\cdot\frac{1}{\exp(\sqrt{2}\sigma y)\sigma\sqrt{2\pi}}\cdot\exp(-y^2)\cdot\sqrt{2}\sigma\exp(\sqrt{2}\sigma y)dy\\
    &=\int_{-\infty}^{\infty}\exp(\sqrt{2}\sigma y)^d\exp(-s\exp(\sqrt{2}\sigma y))\cdot\frac{1}{\sqrt{\pi}}\exp(-y^2)dy
\end{align}
\begin{defn}[Gauss-Hermite Quadrature]\label{defn:gausshermite}
    The integrand part can be solved using Gauss-Hermite Quadrature. In numerical analysis, the method can be applied in the following form:
\begin{equation}
    \int_{-\infty}^{\infty}\exp(-x^2)f(x)dx\approx \sum_{i=1}^n\omega_i f(x_i)
\end{equation}
where $n$ is number of sample points used, and $x_i$ is the roots of Hermite polnomial $H_n (x)$ such that $i=1, ..., n$, and the weights $\omega_i$ is 
\begin{equation}
    \omega_i=\frac{2^{n-1}n!\sqrt{n}}{n^2[H_{n-1}(x_i)]^2}
\end{equation}
\end{defn}

\noindent
Applying Definition~\ref{defn:gausshermite}, the integral of the Laplace transform is then
\begin{equation}
    \phi(s)^d=\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\omega_{q}\exp(-s\exp(\sqrt{2}\sigma y_{q}))\exp(\sqrt{2}\sigma y_{q})^d
\end{equation}
where $q$ denotes the $q$-th element of Gauss Hermite Quadrature, i.e., $\omega_{q}$ denotes the $q$-th weight, $y_{q}$ denotes the $q$-th node, and $N_{q}$ denotes the total number of quadratures. Thus, substituting into the log-likelihood:
\begin{equation}
    \ell_j(\boldsymbol{\theta})=\sum_{i=1}^{n_j}\delta_{ij}\log(h(t_{ij}|\mathbf{x}_{ij}))+\log\Big (\frac{1}{\sqrt{\pi}}\sum_{q=1}^{N_{q}}\left [\omega_{q}\exp(\sqrt{2}\sigma y_{q})^{d_j}\exp\Big (-\sum_{i=1}^{n_j}H(t_{ij}|\mathbf{x}_{ij})\exp(\sqrt{2}\sigma y_{q})\Big )\right ]\Big )
\end{equation}
Similarly, the ascertainment correction in the log-normal frailty can be written as 
\begin{align}
    A_j(\boldsymbol{\theta})&=\Big [1-\int_{-\infty}^{\infty} \exp(-z H(a_{p_j}|\mathbf{x}_{p_j}))f(z)dz\Big ]^{I(T_{p_j}<a_{p_j})}\Big [ \int_{-\infty}^{\infty} \exp(-z H(a_{p_j}|\mathbf{x}_{p_j}))f(z)dz\Big ]^{1-I(T_{p_j}<a_{p_j})}\\
    &=\Big [1-\sum_{q=1}^{N_{q}}\omega_{q} \exp\left (-(\sum_{i=1}^{n_j} H(a_{p_j}|\mathbf{x}_{p_j}))\exp (\sqrt{2}\sigma y_{q_p})\right )\Big ]^{I(T_{p_j}<a_{p_j})}\\
    &\times \Big [ \sum_{q=1}^{N_{q}}\omega_{q} \exp\left (-(\sum_{i=1}^{n_j} H(a_{p_j}|\mathbf{x}_{p_j}))\exp (\sqrt{2}\sigma y_{q_p}) \right ) \Big ]^{1-I(T_{p_j}<a_{p_j})}
\end{align}

\section{Likelihood and Missing Data}\label{sec:likeandmissing}
\subsection{Reviews on Missing Data}
In this subsection, the notations are \textbf{distinct} to all other sections or subsections. 
The missing data problem was firstly brought by~\citet{rubin1976inference}, and further targetted as a major statistical problem which many methodologists have developed different statistical tools to handle the missing data. 
Such as the practical book written by~\citet{Rubin1987}, and some comprehensive reviews on current missing data problems by~\citet{baraldi2010introduction}. 
The missing data mechanism was introduced by~\citet{little2019statistical}. 
There are three missing data mechanisms, which are Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR). 
There are some reviews on the missing data which rigorously present the statistical concept of three types of the missing mechanism~\cite{santos2019generating}.
\begin{defn}(MCAR)\label{defn:MCAR}
Denote $Y$ as the complete data matrix, and $M$ as the missing data indicator matrix. 
Define $y_{ij}$ and $m_{ij}$ as $i$-th row (observation) and $j$-th column (variable) for the matrix $Y$ and $M$. 
The conditional distribution of the missingness is said to be 
\begin{equation} 
    f(m_i|y_i, \phi)=f(m_i|\phi)
\end{equation}
That is, for the parameters of this distribution, $m_i$ does not depend on any observed or missing data.
\end{defn}
\begin{eg}(MCAR Example)\label{eg:MCARExample}
    There is a blind box with 500 indexed balls (No. 1 to 500) and their weights are unknown. 
    We randomly draw 100 balls and measure their weights and record them in the Excel file. 
    The Excel file contains two columns called Index and Weight, only those randomly selected balls will have Weights being filled. 
    Those weights of unselected balls are called MCAR. 
\end{eg}
\begin{defn}(MAR)\label{defn:MAR}
    Denote $y_{i,obs}$ as the observed $y$, and $y_{i,mis}$ as the missing $y$. 
    Note that $y_i=(y_{i,obs},y_{i,mis})$. 
    The missing component is defined to be MAR if $m$ only dependes on $y_{i,obs}$. 
    That is,

    \begin{equation} 
        f(m_i|y_i,\phi)=f(m_i|y_{i,obs}, \phi)
    \end{equation}
\end{defn}
\begin{eg}(MAR Example)\label{eg:MARExample}
    In a psychological study, participants are asked to complete a survey so the scientist can profile their personalities. 
    One question that asks participants to report their Mood status being good or bad. 
    Male participants are typically too shy to answer this question, which yields some responses being missing. 
    This is called the MAR, that the missingness on Mood status depends on the participant's gender, but not on the missing Mood itself.
\end{eg}
\begin{defn}(MNAR)\label{defn:MNAR}
    In the MNAR, the missingness depends on the missing data itself, which is 
    \begin{equation} 
        f(m_i|y_i,\phi)=f(m_i|y_{i,mis}, y_{i,obs}, \phi)
    \end{equation}
    In this case, the analysis needs to be conducted with caution. 
    The missingness should be included in the likelihood construction.
\end{defn}
\begin{eg}(MNAR Example)\label{eg:MNARExample}
    There is a study on participants' incomes. 
    Person A makes \$200,000 per year, so they decide to report this amount without hesitancies. 
    Person B makes \$10,000 per year, so they are not willing to provide this information, which this response is left as blank. 
    This type of missing depends on the missing data itself, that Person B refuses to provide the response due to the response being comparatively low. 
\end{eg}
 

\subsection{Multiple Imputation for the Continuous Variable without Considering the Family Structure}
Again, to avoid overloaded mathematical notations, this subsection will not follow the previously defined notations. 
When making the imputation on the continuous variable, one easy way is to assume a conditionally normal model. 
Suppose $y$ is the variable contains missing values, $\mathbf{x}$ are other variables are fully observed. 
We assume there are total $p$ parameters being estimated in this linear regression. 
The conditionally normal model (linear regression) can be written as 
\begin{equation} 
    y|\mathbf{x},\boldsymbol{\beta}\sim N(\boldsymbol{\beta}\mathbf{x}, \sigma^2)
\end{equation}
In the linear regression setting, 
\begin{equation} 
    y=\mathbf{x}\boldsymbol{\beta}+\epsilon,~ \epsilon\sim N(0, \sigma^2)
\end{equation}
This simply corresponds to the likelihood 
\begin{equation}\label{eq:bayeslinearnormal}
    f(y|\mathbf{x}, \boldsymbol{\beta}, \sigma^2)\propto (\sigma^2)^{-n/2}\exp \Big (-\frac{1}{2\sigma^2} (y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})\Big )
\end{equation}
because of the conditional normality of $y$. 
We can solve that 
\begin{equation} 
    \hat{\boldsymbol{\beta}}=(\mathbf{x}^{\top}\mathbf{x})^{-1}\mathbf{x}^{\top}y
\end{equation}
In the Bayesian framework, we need to find the prior, which is the joint distribution $f(\sigma^2,\boldsymbol{\beta})$. 
Note that $(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})$ can be written as 
\begin{align} 
    (y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta})&=\big ((y-\mathbf{x}\boldsymbol{\beta})+(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\big )^{\top}\big ((y-\mathbf{x}\boldsymbol{\beta})+(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\big )\\
    &=(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})+2(\mathbf{x}\hat{\boldsymbol{\beta}}-\mathbf{x}\boldsymbol{\beta})\\
    &=(y-\mathbf{x}\boldsymbol{\beta})^{\top}(y-\mathbf{x}\boldsymbol{\beta}) + (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})
\end{align}
So Equation~\ref{eq:bayeslinearnormal} will become 
\begin{equation} 
    f(y|\mathbf{x}, \boldsymbol{\beta}, \sigma^2)\propto \underbrace{(\sigma^2)^{-v/2}\exp (-\frac{vs^2}{2\sigma^2})}_{f(\sigma)}\underbrace{(\sigma^2)^{-\frac{n-v}{2}}\exp \Big (-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\top}(\mathbf{x}^{\top}\mathbf{x})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})\Big )}_{f(\boldsymbol{\beta}|\sigma)}
\end{equation}
where $vs^2=(\mathbf{y}-\mathbf{x}\hat{\boldsymbol{\beta}})^{\top}(\mathbf{y}-\mathbf{x}\hat{\boldsymbol{\beta}})=SSE$, such that $v=n_{obs}-p$. 
Then $f(\sigma^2)$ can be written as a proportional density to the inverse gamma distribution,
\begin{equation} 
    f(\sigma^2)\propto (\sigma^2)^{-\frac{v}{2}-1}\exp (-\frac{vs^2}{2\sigma^2})=(\sigma^2)^{-\frac{v}{2}-1}\exp (-\frac{SSE}{2\sigma^2})
\end{equation}
In this $\sigma^2\sim \text{Inverse-Gamma}(\alpha, \phi)$, we have $\alpha = \frac{v}{2}=\frac{n_{obs}-p}{2}$ and $\phi = \frac{1}{2}vs^2=\frac{SSE}{2}$.
From the inverse-gamma property, when 
\begin{equation} 
    \sigma^2\sim \text{Inverse-Gamma}(\frac{n_{obs}-p}{2}, \frac{SSE}{2})
\end{equation}
and $\exists\lambda$ such that 
\begin{equation} 
    \sigma^2=\frac{SSE}{2}/\lambda
\end{equation}
then $\lambda$ can be transformed 
\begin{equation} 
    \lambda = \frac{\chi^2_{n_{obs}-p}}{2}
\end{equation}
since $\chi^2_{df}=\text{Gamma}(\frac{df}{2},2)$, so 
\begin{equation} 
    \sigma^2=\frac{\frac{SSE}{2}}{\frac{\chi^2_{n_{obs}-p}}{2}}=\frac{SSE}{\chi^2_{n_{obs}-p}}
\end{equation}
Thus, we can sample the standard deviation of the missing data distribution from 
\begin{equation} 
    \sigma^*=\hat{\sigma}\sqrt{\frac{SSE}{\chi^2_{n_{obs}-p}}}=\hat{\sigma}\sqrt{\frac{SSE}{g}}
\end{equation}
from sampling $g\sim \chi^2_{n_{obs}-p}$. 
Moreover, we know 
\begin{equation} 
    \text{Var}(\hat{\boldsymbol{\beta}})=\sigma^2(\mathbf{x}^{\top}\mathbf{x}^{-1})=\mathbf{V}
\end{equation}
so the marginal distribution for $\boldsymbol{\beta}$, 
\begin{equation}\label{eq:betadistmi}
    \boldsymbol{\beta}\sim N(\hat{\boldsymbol{\beta}}, \sigma^2(\mathbf{x}^{\top}\mathbf{x})^{-1})
\end{equation}
Note that when a random variable $T\sim N(\mathbf{m}, \mathbf{c})$, then $T$ can be generated from a standard normal variable $\mathbf{u}$ with 
\begin{equation} 
    T=\mathbf{m}+\mathbf{L}\mathbf{u}
\end{equation}
where $\mathbf{L}$ is the cholesky decomposition of $\mathbf{c}$ such that $\mathbf{c}=\mathbf{L}\mathbf{L}^{\top}$.
For $\boldsymbol{\beta}$, from~\ref{eq:betadistmi}, it can be derived as 
\begin{align} 
    \boldsymbol{\beta}&=\hat{\boldsymbol{\beta}}+\sigma (\mathbf{x}^{\top}\mathbf{x})^{-1/2}\mathbf{u}_1\\
    &=\hat{\boldsymbol{\beta}}+\mathbf{u}_1\mathbf{V}^{-1/2}
\end{align}
Adjusting for $\sigma^*$ to make sure $\boldsymbol{\beta}$ matches the variability implied by the random draw of $\sigma^*$, 
\begin{equation} 
    \boldsymbol{\beta}^*=\hat{\boldsymbol{\beta}}+\frac{\sigma^*}{\hat{\sigma}}\mathbf{u}_1\mathbf{V}^{-1/2}
\end{equation}
such that $\mathbf{u}_1$ is a row vector of $p$ independent draws from a standard normal distribution, $u_{1k}\stackrel{iid}{\sim} N(0,1)$, and $k=1,...,p$. 
The imputation for $y_i^*$ is computed as 
\begin{equation} 
    y_i^*=\boldsymbol{\beta}^*\mathbf{x}_i+u_{2i}\sigma^*,~ s.t.~ u_{2i}\sim N(0,1)
\end{equation}
where $u_{2i}$ adds the uncertainty to the imputation as well to ensure the imputation is not solely based on the predicted value of $y_i^*$. 
This prevents the underestimation of the variability. 
Therefore, the comprehensive steps of the multiple imputation on the continuous missing data can be summarized to the following steps: 
\begin{enumerate} 
    \item Calculate $\hat{y}=\hat{\boldsymbol{\beta}}\mathbf{x}$ using $y_{obs}$, and $\hat{\boldsymbol{\beta}}$ can be obtained easily, as well as $\hat{\sigma}$, and $\text{Var}(\hat{\boldsymbol{\beta}})=\mathbf{V}$
    \item Draw $g\sim \chi^2_{n_{obs}-p}$ for one random draw 
    \item Calculate $\sigma^*=\hat{\sigma}/\sqrt{SSE/g}$
    \item Draw a $p$ dimensional vector $\mathbf{u}_1$ such that $u_{1k}\stackrel{iid}{\sim} N(0,1)$ and $k=1,...,p$
    \item Calculate $\boldsymbol{\beta}^*=\hat{\boldsymbol{\beta}}+\frac{\sigma^*}{\hat{\sigma}}\mathbf{u}_1\mathbf{V}^{1/2}$ such that $\mathbf{V}^{1/2}$ is the cholesky decomposition of $\mathbf{V}$
    \item Draw $u_{2i}\stackrel{iid}{\sim} N(0,1)$ 
    \item Impute $y_{mis,i}=\boldsymbol{\beta}^*\mathbf{x}_i+u_{2i}\sigma^*$ 
    \item Repeat 2. to 7. for $M$ times to obtain $M$ complete datasets
\end{enumerate}

\subsection{Multiple Imputation for the Continuous Variable Considering the Family Structure}
In order to account for the kinship correlations, the conditional normal distribution needs to be adjusted where the variable contains the missing components are said to be multivariate 
\begin{equation} 
    \mathbf{y}|\mathbf{x},\boldsymbol{\beta}\sim MVN(\boldsymbol{\beta}\mathbf{x}, \sigma_g^2K+\sigma_e^2)
\end{equation}
where in the linear mixed effect regression form with flexible covariance matrix, the model can be written as 
\begin{equation} 
    \mathbf{y}=\mathbf{x}\boldsymbol{\beta}+\mathbf{u}+\mathbf{e}
\end{equation}
where $\mathbf{u}\sim MVN(0,\sigma_g^2K)$, and $\mathbf{e}\sim MVN(0,\sigma_e^2I)$, such that $K$ is the kinship matrix with the diagonal of 1 and $I$ is the identity matrix. 
Denote that $\boldsymbol{\Sigma}=\sigma_g^2K+\sigma_e^2$. 
So in this multivariate version of linear mixed effects model. 
With unknown mean and covariance matrix, the prior of the covariance matrix can be selected as an Inverse Wishart distribution with some degrees of freedom and the scale parameter. 
However, in this case, the covariance matrix is not fully unknown. 
Therefore, some steps of this multiple imputation will be based on the empirical estimates rather than the prior distributions. 
The step can be concluded as follows: 
\begin{enumerate} 
    \item Obtain the kinship matrix $K$ among all individuals 
    \item Calculate the estimates of $\hat{y}=\mathbf{x}\boldsymbol{\beta}$, obtain estimates of $\hat{\boldsymbol{\beta}}$, $\hat{\sigma_g}^2$, $\hat{\sigma_e}^2$, $\text{Var}(\hat{\boldsymbol{\beta}})=\mathbf{V}$. In this step, naturally, $\boldsymbol{\hat{\Sigma}}$ is obtained.
    \item Obtain the conditional variance of $y_i$, 
    \begin{equation} 
        \text{Var}(y_i|\mathbf{y}_{-i})=\hat{\Sigma}_{ii}-\hat{\boldsymbol{\Sigma}}_{i,-i}\hat{\boldsymbol{\Sigma}}_{-i,-i}^{-1}\hat{\boldsymbol{\Sigma}}_{-i,i}=\hat{\sigma_i}^2
    \end{equation}
    \item Draw $p$-dimensional vector $w_1$ such that $w_{1k}\stackrel{iid}{\sim} N(0,1)$ where $k=1,...,p$
    \item Calculate $\boldsymbol{\beta}^*=\hat{\boldsymbol{\beta}}+w_1\mathbf{V}^{1/2}$ such that $\mathbf{V}^{1/2}$ is the cholesky decomposition of $\mathbf{V}$
    \item Obtain $\mu_i^*=\boldsymbol{\beta}^*\mathbf{x}_i$
    \item Obtain the conditional expectations 
    \begin{equation}
        E(y_{mis,i}|\mathbf{y}_{-i})=\mu_i^*+\hat{\boldsymbol{\Sigma}}_{i,-i}\hat{\boldsymbol{\Sigma}}_{-i,-i}^{-1}(\mathbf{y}_{-i}-\boldsymbol{\mu}^*_{-i})
    \end{equation}
    \item Draw $w_{2i}\sim N(0,1)$
    \item Impute $y_{mis,i}=E(y_{mis,i}|\mathbf{y}_{-i}) + w_{2i}\hat{\sigma}_i$
    \item Repeat 4. to 9. for $M$ times to obtain $M$ complete datasets.
\end{enumerate}

\section{Variance Estimation} 
Based on the Rubin's rule, the pooling step of the analysis is defined as 
\begin{equation} 
    \bar{\theta}=\frac{1}{M}\sum_{i=1}^M\theta_i
\end{equation}
where $\theta_i$ is the parameter estimate that we are making the inference in a study for $i$-th imputation after $M$ imputations. 
So $\bar{\theta}$ is the pooled parameter estimates. 
Based on~\citet{barnard1999miscellanea}, the variance estimation is also defined in a two-level structure, which are within imputation variance and between imputation variance. 
The within imputation variance is similar to the pooled parameter estimation, where 
\begin{equation} 
    V_W=\frac{1}{M}\sum_{i=1}^mSE_i^2
\end{equation}
such that $V_W$ determines the within-imputation variance, and it's simply ``pooled" variance among all analyses from the imputed dataset $i$. 
The between-imputation variance, on the other hand, accounts for the extra variances caused by the missing data. 
The between-imputation variance $V_B$ is defined as 
\begin{equation} 
    V_B=\frac{\sum_{i=1}^M(\theta_i-\bar{\theta})^2}{M-1}
\end{equation}
where $V_B$ is the unbiased estimation. 
Note that $\theta$ is estimated using the only finite $M$ imputed datasets, according to~\citet{van2018flexible}, $V_B$ is the approximation when $M\rightarrow\infty$.
Therefore, the total variance can be written as
\begin{equation} 
    V_{\text{Total}}=V_W+V_B+\frac{V_B}{M}
\end{equation}
when $M$ is large enough, $V_{\text{Total}}$ tends to have the only two components of within and between variances. 
The test statistic (Wald statistic) is 
\begin{equation} 
    \frac{(\bar{\theta}-\theta_0)^2}{V_{\text{Total}}}\sim F_{1,df_{adj}}
\end{equation}
which 
\begin{equation} 
    \frac{\bar{\theta}-\theta_0}{\sqrt{V_{\text{Total}}}}\sim t_{df_{adj}, \frac{1-\alpha}{2}}
\end{equation}
such that $\alpha$ is the significance level. 
Define 
\begin{equation}
    \rho=\frac{V_B+\frac{V_B}{M}}{V_{\text{Total}}}
\end{equation}
and 
\begin{equation}
    r=\frac{V_B+\frac{V_B}{M}}{V_W}
\end{equation}
where $\rho$ is the fraction of missing information, which quantifies the proportion of the total variance that is attributable to the fact that the data have been imputed $M$ times. 
Also, $r$ is the relative increase in variance due to non-responses (missing data), which measures the increase in variance due to the missing data compared to if there were no missing data. 
In the interpretation, $\rho$ gives the proportion of the total variance that is due to the uncertainty introduced by the missing data. 
A higher $\rho$ indicates a larger fractino of the total uncertainty comes from the fact that the data were imputed.
A higher $r$ indicates that the missing data have a larger impact on the overall variance. 
In the first MI literature by~\citet{Rubin1987Multiple}, the degrees of freedom was defined as
\begin{equation}\label{eq:dfold}
    df_{old}=(M-1)\times (1+\frac{1}{r})^2
\end{equation}
The estimated degrees of freedom for the observed data, adjusted for the missing information, are
\begin{equation}\label{eq:dfobs}
    df_{obs}=\frac{(n-k)+1}{(n-k)+3}(n-k)(1-\rho)
\end{equation}
where $n$ is the number of observations in each imputed data, and $k$ is the number of parameters to be estimated.
\citet{barnard1999miscellanea} further alternated the calculation of the degrees of freedom by combining Equations~\ref{eq:dfold} and~\ref{eq:dfobs}
\begin{equation}
    df_{adj}=\frac{df_{old}df_{obs}}{df_{old}+df_{obs}}
\end{equation}
When conducting the statistical tests on the pooled estimates, $df_{adj}$ is used for the t-distribution degrees of freedom. 
The confidence interval is straightforward, where we obtain 
\begin{equation}
    SE_{pooled}=\sqrt{V_{\text{Total}}}
\end{equation}
then we simply calculate 
\begin{equation}
    CI=\bar{\theta}\pm t_{df_{adj},\frac{1-\alpha}{2}}\times SE_{pooled}
\end{equation}


\section{Simulation Study (Temporary)}
\subsection{Generating Missing at Random}
From the MICE (Multivariate Imputation by Chained Equations) package in R authored by~\citet{van2011mice}, there is one function add-on that is very helpful in generating the missingness of a data while considering the missing mechanism. 
The function was introduced by~\citet{schouten2018generating} that on the opposite of imputation, the amputation is designed to simulate the missing data. 

%\section{Stochastic EM?}




\newpage

%\section{References}
%\label{S.7}
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

%\bibliographystyle{elsarticle-num}
\bibliographystyle{unsrtnat}
\bibliography{progress.bib}
%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

%\begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

%\bibitem{}

%\end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.